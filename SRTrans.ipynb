{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "816da1e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-03T01:20:54.418574Z",
     "start_time": "2022-10-03T01:20:53.935224Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-09T08:33:08.650476Z",
     "iopub.status.busy": "2023-03-09T08:33:08.649861Z",
     "iopub.status.idle": "2023-03-09T08:33:09.170207Z",
     "shell.execute_reply": "2023-03-09T08:33:09.169445Z",
     "shell.execute_reply.started": "2023-03-09T08:33:08.650416Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.distributions.gumbel import *\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sp\n",
    "import datetime\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17e4170e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-03T01:20:55.225232Z",
     "start_time": "2022-10-03T01:20:55.217811Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-09T08:33:09.979585Z",
     "iopub.status.busy": "2023-03-09T08:33:09.978995Z",
     "iopub.status.idle": "2023-03-09T08:33:09.985755Z",
     "shell.execute_reply": "2023-03-09T08:33:09.984842Z",
     "shell.execute_reply.started": "2023-03-09T08:33:09.979536Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_torch(seed=6298):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    #torch.cuda.manual_seed(seed)\n",
    "    #torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    #torch.backends.cudnn.enabled = False\n",
    "seed_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd186d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-03T01:21:00.641512Z",
     "start_time": "2022-10-03T01:21:00.485564Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-09T08:33:10.690866Z",
     "iopub.status.busy": "2023-03-09T08:33:10.690183Z",
     "iopub.status.idle": "2023-03-09T08:33:10.847768Z",
     "shell.execute_reply": "2023-03-09T08:33:10.846168Z",
     "shell.execute_reply.started": "2023-03-09T08:33:10.690802Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0,1\n",
    "cuda = torch.device('cuda') \n",
    "#torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8156dbf",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f907e2b-9ffe-44dd-8b19-c842df414fb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-03T04:06:42.146879Z",
     "start_time": "2022-10-03T04:06:41.636594Z"
    },
    "code_folding": [
     438,
     459
    ],
    "execution": {
     "iopub.execute_input": "2023-03-09T07:35:25.209316Z",
     "iopub.status.busy": "2023-03-09T07:35:25.208725Z",
     "iopub.status.idle": "2023-03-09T07:35:25.296433Z",
     "shell.execute_reply": "2023-03-09T07:35:25.295822Z",
     "shell.execute_reply.started": "2023-03-09T07:35:25.209262Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_sp_mat_to_sp_tensor(X):\n",
    "    coo = X.tocoo().astype(np.float32)\n",
    "    row = torch.Tensor(coo.row).long()\n",
    "    col = torch.Tensor(coo.col).long()\n",
    "    index = torch.stack([row, col])\n",
    "    data = torch.FloatTensor(coo.data)\n",
    "    return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))\n",
    "\n",
    "def getSparseGraph(n_users, m_items, Network):\n",
    "    #print(\"generating adjacency matrix\")\n",
    "    adj_mat = sp.dok_matrix((n_users + m_items, n_users + m_items), dtype=np.float32)\n",
    "    adj_mat = adj_mat.tolil()\n",
    "    R = Network.tolil()\n",
    "    adj_mat[:n_users, n_users:] = R\n",
    "    adj_mat[n_users:, :n_users] = R.T\n",
    "    adj_mat = adj_mat.todok()\n",
    "    \n",
    "    rowsum = np.array(adj_mat.sum(axis=1))\n",
    "    d_inv = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv[np.isinf(d_inv)] = 0.\n",
    "    d_mat = sp.diags(d_inv)\n",
    "    \n",
    "    norm_adj = d_mat.dot(adj_mat)\n",
    "    norm_adj = norm_adj.dot(d_mat)\n",
    "    norm_adj = norm_adj.tocsr()\n",
    "    \n",
    "    Graph = convert_sp_mat_to_sp_tensor(norm_adj)\n",
    "    Graph = Graph.coalesce()\n",
    "    return Graph\n",
    "\n",
    "# Evaluation\n",
    "def RecallPrecision_ATk(test_data, r, k):\n",
    "    \"\"\"\n",
    "    test_data should be a list? cause users may have different amount of pos items. shape (test_batch, k)\n",
    "    pred_data : shape (test_batch, k) NOTE: pred_data should be pre-sorted\n",
    "    k : top-k\n",
    "    \"\"\"\n",
    "    right_pred = r[:, :k].sum(1)\n",
    "    precis_n = k\n",
    "    recall_n = np.array([len(test_data[i]) for i in range(len(test_data))])\n",
    "    recall = np.sum(right_pred/recall_n)\n",
    "    precis = np.sum(right_pred)/precis_n\n",
    "    return {'recall': recall, 'precision': precis}\n",
    "\n",
    "def NDCGatK_r(test_data,r,k):\n",
    "    \"\"\"\n",
    "    Normalized Discounted Cumulative Gain\n",
    "    rel_i = 1 or 0, so 2^{rel_i} - 1 = 1 or 0\n",
    "    \"\"\"\n",
    "    assert len(r) == len(test_data)\n",
    "    pred_data = r[:, :k]\n",
    "\n",
    "    test_matrix = np.zeros((len(pred_data), k))\n",
    "    for i, items in enumerate(test_data):\n",
    "        length = k if k <= len(items) else len(items)\n",
    "        test_matrix[i, :length] = 1\n",
    "    max_r = test_matrix\n",
    "    idcg = np.sum(max_r * 1./np.log2(np.arange(2, k + 2)), axis=1)\n",
    "    dcg = pred_data*(1./np.log2(np.arange(2, k + 2)))\n",
    "    dcg = np.sum(dcg, axis=1)\n",
    "    idcg[idcg == 0.] = 1.\n",
    "    ndcg = dcg/idcg\n",
    "    ndcg[np.isnan(ndcg)] = 0.\n",
    "    return np.sum(ndcg)\n",
    "\n",
    "def getLabel(test_data, pred_data):\n",
    "    r = []\n",
    "    for i in range(len(test_data)):\n",
    "        groundTrue = test_data[i]\n",
    "        predictTopK = pred_data[i]\n",
    "        pred = list(map(lambda x: x in groundTrue, predictTopK))\n",
    "        pred = np.array(pred).astype(\"float\")\n",
    "        r.append(pred)\n",
    "    return np.array(r).astype('float')\n",
    "\n",
    "def test_one_batch(X):\n",
    "    sorted_items = X[0].numpy()\n",
    "    groundTrue = X[1]\n",
    "    r = getLabel(groundTrue, sorted_items)\n",
    "    pre, recall, ndcg = [], [], []\n",
    "    for k in config[\"topks\"]:\n",
    "        ret = RecallPrecision_ATk(groundTrue, r, k)\n",
    "        pre.append(ret['precision'])\n",
    "        recall.append(ret['recall'])\n",
    "        ndcg.append(NDCGatK_r(groundTrue,r,k))\n",
    "    return {'recall':np.array(recall), \n",
    "            'precision':np.array(pre), \n",
    "            'ndcg':np.array(ndcg)}\n",
    "\n",
    "def Test(if_eval):\n",
    "    Recmodel.eval()\n",
    "    u_batch_size = config['test_u_batch_size']\n",
    "    max_K = max(config[\"topks\"])\n",
    "    multicore = config['multicore']\n",
    "    if multicore == 1:\n",
    "        pool = multiprocessing.Pool(12)\n",
    "    results = {'precision': np.zeros(len(config[\"topks\"])),\n",
    "               'recall': np.zeros(len(config[\"topks\"])),\n",
    "               'ndcg': np.zeros(len(config[\"topks\"]))}\n",
    "    with torch.no_grad():\n",
    "        users = list(dict_interactions.keys())\n",
    "        users_list = []\n",
    "        rating_list = []\n",
    "        groundTrue_list = []\n",
    "        total_batch = len(users) // u_batch_size + 1\n",
    "        for batch_users in minibatch(np.arange(n_users_t), batch_size=u_batch_size):\n",
    "            allPos = getUserPosItems(batch_users)\n",
    "            if if_eval:\n",
    "                groundTrue = [dict_interactions[u][-2:-1] for u in batch_users]\n",
    "            else:\n",
    "                groundTrue = [dict_interactions[u][-1:] for u in batch_users]\n",
    "            batch_users = torch.Tensor(batch_users).long()\n",
    "            \n",
    "            rating = Recmodel.getUsersRating_t(batch_users.cuda())\n",
    "            #rating = Recmodel.getUsersRating_t(batch_users)\n",
    "            \n",
    "            exclude_index = []\n",
    "            exclude_items = []\n",
    "            for range_i, items in enumerate(allPos):\n",
    "                exclude_index.extend([range_i] * len(items))\n",
    "                exclude_items.extend(items)\n",
    "            rating[exclude_index, exclude_items] = -(1<<10)\n",
    "            _, rating_k = torch.topk(rating, k=max_K)\n",
    "            \n",
    "            del rating\n",
    "            users_list.append(batch_users)\n",
    "            rating_list.append(rating_k)\n",
    "            groundTrue_list.append(groundTrue)\n",
    "        X = zip(rating_list, groundTrue_list)\n",
    "        if multicore == 1:\n",
    "            pre_results = pool.map(test_one_batch, X)\n",
    "        else:\n",
    "            pre_results = []\n",
    "            for x in X:\n",
    "                pre_results.append(test_one_batch(x))\n",
    "        for result in pre_results:\n",
    "            results['recall'] += result['recall']\n",
    "            results['precision'] += result['precision']\n",
    "            results['ndcg'] += result['ndcg']\n",
    "        results['recall'] /= float(len(users))\n",
    "        results['precision'] /= float(len(users))\n",
    "        results['ndcg'] /= float(len(users))\n",
    "        \n",
    "        if multicore == 1:\n",
    "            pool.close()\n",
    "        #print(results)\n",
    "        return results\n",
    "\n",
    "def minibatch(*tensors, batch_size):\n",
    "\n",
    "    if len(tensors) == 1:\n",
    "        tensor = tensors[0]\n",
    "        for i in range(0, len(tensor), batch_size):\n",
    "            yield tensor[i:i + batch_size]\n",
    "    else:\n",
    "        for i in range(0, len(tensors[0]), batch_size):\n",
    "            yield tuple(x[i:i + batch_size] for x in tensors)\n",
    "\n",
    "def getUserPosItems(batch_users):\n",
    "    posItems = []\n",
    "    for user in batch_users:\n",
    "        posItems.append(dict_interactions[user][:-2])\n",
    "    return posItems\n",
    "\n",
    "def shuffle(*arrays, **kwargs):\n",
    "\n",
    "    require_indices = kwargs.get('indices', False)\n",
    "\n",
    "    if len(set(len(x) for x in arrays)) != 1:\n",
    "        raise ValueError('All inputs to shuffle must have '\n",
    "                         'the same length.')\n",
    "\n",
    "    shuffle_indices = np.arange(len(arrays[0]))\n",
    "    np.random.shuffle(shuffle_indices)\n",
    "\n",
    "    if len(arrays) == 1:\n",
    "        result = arrays[0][shuffle_indices]\n",
    "    else:\n",
    "        result = tuple(x[shuffle_indices] for x in arrays)\n",
    "\n",
    "    if require_indices:\n",
    "        return result, shuffle_indices\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "def UniformSample_original():\n",
    "    \"\"\"\n",
    "    the original impliment of BPR Sampling in LightGCN\n",
    "    :return:\n",
    "        np.array\n",
    "    \"\"\"\n",
    "    user_num_t = len(tr_u_t) - len(tr_u_t)%config[\"bpr_batch_size\"]\n",
    "    users_t = np.random.randint(0, n_users_t, user_num_t)\n",
    "    users_s = np.random.randint(0, n_users_s, user_num_t)\n",
    "    S = []\n",
    "    for i, user in enumerate(users_t):\n",
    "        posForUser = dict_interactions[user][:-2]\n",
    "        if len(posForUser) == 0:\n",
    "            continue\n",
    "        posindex = np.random.randint(0, len(posForUser))\n",
    "        positem = posForUser[posindex]\n",
    "        while True:\n",
    "            negitem = np.random.randint(0, m_items_t)\n",
    "            if negitem in posForUser:\n",
    "                continue\n",
    "            else:\n",
    "                break           \n",
    "        \n",
    "        posForUser_s = dict_interactions_s[users_s[i]]\n",
    "        if len(posForUser_s) == 0:\n",
    "            continue\n",
    "        posindex = np.random.randint(0, len(posForUser_s))\n",
    "        positem_s = posForUser_s[posindex]\n",
    "        while True:\n",
    "            negitem_s = np.random.randint(0, m_items_s)\n",
    "            if negitem in posForUser:\n",
    "                continue\n",
    "            else:\n",
    "                break                 \n",
    "        S.append([user, positem, negitem, users_s[i], positem_s, negitem_s])\n",
    "    return np.array(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50038267-f6db-4e7e-b2da-a56cd648f72b",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c4e2a35-e341-4d5d-a933-046fde8fbe67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-03T04:06:42.146879Z",
     "start_time": "2022-10-03T04:06:41.636594Z"
    },
    "code_folding": [
     438,
     459
    ],
    "execution": {
     "iopub.execute_input": "2023-03-09T07:35:25.209316Z",
     "iopub.status.busy": "2023-03-09T07:35:25.208725Z",
     "iopub.status.idle": "2023-03-09T07:35:25.296433Z",
     "shell.execute_reply": "2023-03-09T07:35:25.295822Z",
     "shell.execute_reply.started": "2023-03-09T07:35:25.209262Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BPRLoss:\n",
    "    def __init__(self):\n",
    "        self.decay_reg = config['decay_reg']     # beta\n",
    "        self.decay_dp = config['decay_dp'] # gamma\n",
    "        self.decay_kt = config['decay_kt'] # alpha\n",
    "        \n",
    "        self.lr = config['lr']\n",
    "        self.opt = optim.Adam(Recmodel.parameters(), lr=self.lr)\n",
    "\n",
    "    def stageOne(self, users, posI, negI, users_s, posI_s, negI_s):\n",
    "        #pred_loss_s, pred_loss_t, dp_loss_s, dp_loss_t , kld_loss_t, reg_loss = Recmodel.bpr_loss(users, posI, negI, \n",
    "        #                                                                                users_s, posI_s, negI_s)\n",
    "        pred_loss_s, pred_loss_t, dp_loss_s, dp_loss_t , kld_loss_t, reg_loss = Recmodel.bpr_loss(users.cuda(), posI.cuda(), negI.cuda(), \n",
    "                                                                                        users_s.cuda(), posI_s.cuda(), negI_s.cuda())\n",
    "        dp_loss_s = dp_loss_s * self.decay_dp\n",
    "        dp_loss_t = dp_loss_t * self.decay_dp\n",
    "        \n",
    "        kld_loss_t =kld_loss_t * self.decay_kt\n",
    "        reg_loss = reg_loss * self.decay_reg\n",
    "        \n",
    "        #loss = pred_loss_s + pred_loss_t + dp_loss_s + dp_loss_t + kld_loss_t + reg_loss\n",
    "        loss = pred_loss_s + pred_loss_t + kld_loss_t + reg_loss\n",
    "        \n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "        return loss.cpu().item(), pred_loss_s.cpu().item(), pred_loss_t.cpu().item(), \\\n",
    "                dp_loss_s.cpu().item(), dp_loss_t.cpu().item(), kld_loss_t.cpu().item(), reg_loss.cpu().item()\n",
    "    \n",
    "def BPR_train_original(epoch):\n",
    "    Recmodel.train()\n",
    "    bpr = BPRLoss()\n",
    "    \n",
    "    T = UniformSample_original()\n",
    "    \n",
    "    users = torch.Tensor(T[:, 0]).long()\n",
    "    posItems = torch.Tensor(T[:, 1]).long()\n",
    "    negItems = torch.Tensor(T[:, 2]).long()\n",
    "    \n",
    "    users_s = torch.Tensor(T[:, 3]).long()\n",
    "    posItems_s = torch.Tensor(T[:, 4]).long()\n",
    "    negItems_s = torch.Tensor(T[:, 5]).long()\n",
    "    \n",
    "    users, posItems, negItems, users_s, posItems_s, negItems_s = shuffle(users, posItems, negItems, users_s, posItems_s, negItems_s)\n",
    "    \n",
    "    total_batch = len(users) // config['bpr_batch_size'] + 1\n",
    "    \n",
    "    aver_loss, aver_pre_loss_s, aver_pre_loss_t, aver_rec_loss_s, aver_rec_loss_t, aver_kt_loss, aver_reg_loss = 0., 0., 0., 0., 0., 0.,0.\n",
    "    for (batch_i, \n",
    "         (batch_users, \n",
    "          batch_posItem, \n",
    "          batch_negItem, \n",
    "          batch_users_s, \n",
    "          batch_posItem_s,\n",
    "          batch_negItem_s)) in enumerate(minibatch(users, \n",
    "                                                    posItems,\n",
    "                                                    negItems, \n",
    "                                                    users_s, \n",
    "                                                    posItems_s, \n",
    "                                                    negItems_s,\n",
    "                                                    batch_size=config['bpr_batch_size'])):\n",
    "        \n",
    "        loss, pred_loss_s, pred_loss_t, dp_loss_s, dp_loss_t , kld_loss_t, reg_loss = bpr.stageOne(batch_users,batch_posItem, batch_negItem, \n",
    "                                                                                         batch_users_s, batch_posItem_s,batch_negItem_s)\n",
    "        aver_loss += loss\n",
    "        aver_pre_loss_s += pred_loss_s\n",
    "        aver_pre_loss_t += pred_loss_t\n",
    "        aver_rec_loss_s += dp_loss_s\n",
    "        aver_rec_loss_t += dp_loss_t\n",
    "        aver_kt_loss += kld_loss_t\n",
    "        aver_reg_loss += reg_loss\n",
    "\n",
    "    aver_loss = aver_loss / total_batch\n",
    "    aver_pre_loss_s = aver_pre_loss_s / total_batch\n",
    "    aver_pre_loss_t = aver_pre_loss_t / total_batch\n",
    "    aver_rec_loss_s = aver_rec_loss_s / total_batch\n",
    "    aver_rec_loss_t = aver_rec_loss_t / total_batch\n",
    "    aver_kt_loss = aver_kt_loss / total_batch\n",
    "    aver_reg_loss = aver_reg_loss / total_batch\n",
    "    \n",
    "    return aver_loss, aver_pre_loss_s, aver_pre_loss_t, aver_rec_loss_s, aver_rec_loss_t, aver_kt_loss, aver_reg_loss\n",
    "\n",
    "# SRTrans\n",
    "\n",
    "class SRTrans(nn.Module):\n",
    "    def __init__(self, \n",
    "                 config:dict,\n",
    "                 UIGraph_s,\n",
    "                 UIGraph_t,\n",
    "                is_GNN):\n",
    "        super(SRTrans, self).__init__()\n",
    "        self.config = config\n",
    "        self.UIGraph_s = UIGraph_s.cuda()\n",
    "        self.UIGraph_t = UIGraph_t.cuda()\n",
    "        self.__init_weight()\n",
    "        self.is_GNN = is_GNN\n",
    "        \n",
    "\n",
    "    def __init_weight(self):\n",
    "        self.n_users_s, self.n_users_t = n_users_s, n_users_t\n",
    "        self.m_items_s, self.m_items_t = m_items_s, m_items_t\n",
    "\n",
    "        self.latent_dim = self.config['latent_dim_rec']\n",
    "        self.n_layers_s = self.config['lightGCN_n_layers_s']\n",
    "        self.n_layers_t = self.config['lightGCN_n_layers_t']\n",
    "        \n",
    "        self.embedding_user_s = torch.nn.Embedding(\n",
    "            num_embeddings=self.n_users_s, embedding_dim=self.latent_dim).cuda()\n",
    "        \n",
    "        self.embedding_item_s = torch.nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(list(dict_ItemIndex2vec_s.values())), freeze=True).cuda()\n",
    "        \n",
    "        self.embedding_user_t = torch.nn.Embedding(\n",
    "            num_embeddings=self.n_users_t, embedding_dim=self.latent_dim).cuda()\n",
    "        \n",
    "        self.embedding_item_t = torch.nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(list(dict_ItemIndex2vec_t.values())), freeze=True).cuda()\n",
    "        \n",
    "        nn.init.normal_(self.embedding_user_s.weight, std=0.1)\n",
    "        nn.init.normal_(self.embedding_user_t.weight, std=0.1)\n",
    "        #-------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        #self.fc_i_s = nn.Linear(768, self.latent_dim)\n",
    "        self.fc_i_t = nn.Linear(768, self.latent_dim).cuda()\n",
    "        #nn.init.normal_(self.fc_i_s.weight, std=0.1)\n",
    "        #nn.init.normal_(self.fc_i_s.bias, std=0.1)  \n",
    "        #nn.init.normal_(self.fc_i_t.weight, std=0.1)\n",
    "        #nn.init.normal_(self.fc_i_t.bias, std=0.1)        \n",
    "        #-------------------------------------------------------------      \n",
    "        \n",
    "        self.f = nn.Sigmoid()\n",
    "        #self.UIGraph_s = getSparseGraph(self.n_users_s, self.m_items_s, UseritemNet_s)\n",
    "        #self.UIGraph_t = getSparseGraph(self.n_users_t, self.m_items_t, UseritemNet_t)\n",
    "        #--------------------------------------------------------------\n",
    "                \n",
    "        #self.l_clust_encoder_s = nn.Linear(self.config['latent_dim_rec'], self.config[\"n_cluster\"])\n",
    "        self.l_clust_encoder_t = nn.Linear(self.config['latent_dim_rec'], self.config['latent_dim_rec']).cuda()\n",
    "        self.l_clust_encoder_t2 = nn.Linear(self.config['latent_dim_rec'], self.config[\"n_cluster\"]).cuda()\n",
    "        #nn.init.normal_(self.l_clust_encoder.weight, std=0.1)\n",
    "        #nn.init.normal_(self.l_clust_encoder.bias, std=0.1)\n",
    "        self.g_k = Gumbel(torch.zeros(config['bpr_batch_size'], self.config[\"n_cluster\"]).cuda(), \n",
    "                          torch.ones(config['bpr_batch_size'], self.config[\"n_cluster\"]).cuda())\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "        \n",
    "        #--------------------------------------------------------------\n",
    "        self.W_pp = nn.Linear(self.config['latent_dim_rec'], self.config['latent_dim_rec'], bias=False).cuda()\n",
    "        #self.W_pe_s = nn.Linear(self.config['latent_dim_rec'], self.config['latent_dim_rec'])\n",
    "        self.W_pe_t = nn.Linear(self.config['latent_dim_rec'], self.config['latent_dim_rec'], bias=False).cuda()\n",
    "        \n",
    "        nn.init.normal_(self.W_pp.weight, std=0.1)\n",
    "        #nn.init.normal_(self.W.bias, std=0.1)       \n",
    "        nn.init.normal_(self.W_pe_t.weight, std=0.1)\n",
    "        #nn.init.normal_(self.W.bias, std=0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        #Dense_layers = [self.fc_i_s, self.fc_i_t, self.l_clust_encoder_s, self.l_clust_encoder_t]\n",
    "        Dense_layers = [self.fc_i_t, self.l_clust_encoder_t, self.l_clust_encoder_t2]\n",
    "        self.initial_denseLayers(Dense_layers)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def initial_denseLayers(self, layer_lst):\n",
    "        for layer in layer_lst:\n",
    "            nn.init.normal_(layer.weight, std=0.1)\n",
    "            nn.init.normal_(layer.bias, std=0.1)\n",
    "        \n",
    "    def computer_s(self):\n",
    "        \"\"\"\n",
    "        propagate methods for lightGCN\n",
    "        \"\"\"       \n",
    "        users_emb = self.embedding_user_s.weight\n",
    "        items_emb = self.fc_i_t(self.embedding_item_s.weight)\n",
    "        \n",
    "\n",
    "        all_emb = torch.cat([users_emb, items_emb])\n",
    "\n",
    "        embs = [all_emb]\n",
    "    \n",
    "        g_droped = self.UIGraph_s\n",
    "        \n",
    "        for layer in range(self.n_layers_s):\n",
    "            all_emb = torch.sparse.mm(g_droped, all_emb)\n",
    "            embs.append(all_emb)\n",
    "        embs = torch.stack(embs, dim=1)\n",
    "        light_out = torch.mean(embs, dim=1)\n",
    "        users, items = torch.split(light_out, [self.n_users_s, self.m_items_s])\n",
    "        '''\n",
    "        users, items = users_emb, items_emb\n",
    "        '''        \n",
    "        return users, items\n",
    "    \n",
    "    def computer_t(self):\n",
    "        \"\"\"\n",
    "        propagate methods for lightGCN\n",
    "        \"\"\"       \n",
    "        users_emb = self.embedding_user_t.weight\n",
    "        items_emb = self.fc_i_t(self.embedding_item_t.weight)\n",
    "        if self.is_GNN:\n",
    "            all_emb = torch.cat([users_emb, items_emb])\n",
    "\n",
    "            embs = [all_emb]\n",
    "\n",
    "            g_droped = self.UIGraph_t\n",
    "\n",
    "            for layer in range(self.n_layers_t):\n",
    "                all_emb = torch.sparse.mm(g_droped, all_emb)\n",
    "                embs.append(all_emb)\n",
    "            embs = torch.stack(embs, dim=1)\n",
    "            light_out = torch.mean(embs, dim=1)\n",
    "            users, items = torch.split(light_out, [self.n_users_t, self.m_items_t])\n",
    "        else:\n",
    "            users, items = users_emb, items_emb\n",
    "        \n",
    "        return users, items\n",
    "    \n",
    "    def cluster(self, x, tao, l_clust_encoder, l_clust_encoder_2):\n",
    "        \n",
    "        alpha = self.relu(l_clust_encoder(x))\n",
    "        alpha = l_clust_encoder_2(alpha)\n",
    "        y = self.soft((alpha+self.g_k.sample())/tao) \n",
    "        #y = y / (y.sum(dim=0)+0.0000001)\n",
    "        \n",
    "        return torch.matmul(y.t(),x)\n",
    "       \n",
    "    def knowledge_transfer(self, p_s, p_t, gamma_kt, W):\n",
    "        # message pass from paper [https://arxiv.org/pdf/1704.01212.pdf]\n",
    "        \n",
    "        h = torch.cat([p_s, p_t]) / gamma_kt\n",
    "        I = torch.eye(h.shape[0]).cuda()\n",
    "        \n",
    "        #Sim = torch.cdist(h,h,p=2)/2\n",
    "        a_norm = h / (h.norm(dim=1)[:, None]+0.0000001)\n",
    "        Sim = torch.mm(a_norm, a_norm.transpose(0,1))\n",
    "        \n",
    "        A = self.soft(Sim) + I\n",
    "        D = torch.diag(1/torch.pow(A.sum(dim=1), 1/2))\n",
    "\n",
    "        L = torch.matmul(torch.matmul(D, A), D)\n",
    "        #L = D - A\n",
    "        h_next = torch.matmul(L,h)\n",
    "        h_next = self.relu(W(h_next))\n",
    "        \n",
    "        p_s_bar, p_t_bar = h_next[:p_s.shape[0]], h_next[p_s.shape[0]:]\n",
    "        return p_s_bar, p_t_bar, A\n",
    "    \n",
    "    def knowledge_transfer_pp(self, p_s, p_t, p0_s, p0_t, gamma_kt, W):\n",
    "        # message pass from paper [https://arxiv.org/pdf/1704.01212.pdf]\n",
    "        \n",
    "        h = torch.cat([p_s, p_t]) / gamma_kt\n",
    "        I = torch.eye(h.shape[0]).cuda()\n",
    "        \n",
    "        #Sim = torch.cdist(h,h,p=2)/2\n",
    "        a_norm = h / (h.norm(dim=1)[:, None]+0.0000001)\n",
    "        Sim = torch.mm(a_norm, a_norm.transpose(0,1))     \n",
    "        \n",
    "        A = self.soft(Sim) + I\n",
    "        D = torch.diag(1/torch.pow(A.sum(dim=1), 1/2))\n",
    "\n",
    "        L = torch.matmul(torch.matmul(D, A), D)\n",
    "        #L = D - A\n",
    "        h0 = torch.cat([p0_s, p0_t]) / gamma_kt\n",
    "        h_next = torch.matmul(L,h0)\n",
    "        h_next = self.relu(W(h_next))\n",
    "        \n",
    "        p_s_bar, p_t_bar = h_next[:p_s.shape[0]], h_next[p_s.shape[0]:]\n",
    "        return p_s_bar, p_t_bar, A\n",
    "        \n",
    "    def getUsersRating_t(self, users):\n",
    "        all_users, all_items = self.computer_t()\n",
    "        \n",
    "        \n",
    "        users_emb = all_users[users.long()] \n",
    "        items_emb = all_items\n",
    "        \n",
    "        rating = self.f(torch.matmul(users_emb, items_emb.t()))\n",
    "        return rating.cpu()\n",
    "    \n",
    "    def getEmbedding_s(self, users, pos_items, neg_items):\n",
    "        all_users, all_items = self.computer_s()\n",
    "        \n",
    "        users_emb= all_users[users.long()]\n",
    "        \n",
    "        pos_emb = all_items[pos_items]\n",
    "        neg_emb = all_items[neg_items]\n",
    "        \n",
    "        users_emb_ego = self.embedding_user_s(users)  \n",
    "        pos_emb_ego = self.fc_i_t(self.embedding_item_s(pos_items))\n",
    "        \n",
    "        return users_emb, pos_emb, neg_emb, users_emb_ego, pos_emb_ego\n",
    "    \n",
    "    def getEmbedding_t(self, users, pos_items, neg_items):\n",
    "        all_users, all_items = self.computer_t()\n",
    "        \n",
    "        users_emb= all_users[users.long()]\n",
    "        \n",
    "        pos_emb = all_items[pos_items]\n",
    "        neg_emb = all_items[neg_items]\n",
    "        \n",
    "        users_emb_ego = self.embedding_user_t(users)  \n",
    "        pos_emb_ego = self.fc_i_t(self.embedding_item_t(pos_items))\n",
    "        \n",
    "        return users_emb, pos_emb, neg_emb, users_emb_ego, pos_emb_ego\n",
    "    \n",
    "    def pred_loss(self, users_emb, pos_emb, neg_emb):\n",
    "        \n",
    "        poscores = torch.mul(users_emb, pos_emb).sum(dim=1)\n",
    "        negcores = torch.mul(users_emb, neg_emb).sum(dim=1)\n",
    "        \n",
    "        loss = torch.mean(nn.functional.softplus(negcores - poscores))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def dp_loss(self, posI, negI, embedding_item, fc_i):\n",
    "        pos_I_ori = embedding_item(posI.long()) \n",
    "        neg_I_ori = embedding_item(negI.long())    \n",
    "        \n",
    "        I_sim_ori = torch.cosine_similarity(pos_I_ori, neg_I_ori, dim=1)\n",
    "        I_sim = torch.cosine_similarity(fc_i(pos_I_ori), fc_i(neg_I_ori), dim=1)  \n",
    "        \n",
    "        loss = (I_sim_ori - I_sim).pow(2).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def kld_loss(self, users_emb, pos_emb, pos_emb_bar):\n",
    "        \n",
    "        y = self.f(torch.mul(users_emb, pos_emb).sum(dim=1))\n",
    "        y_bar = self.f(torch.mul(users_emb, pos_emb_bar).sum(dim=1))\n",
    "\n",
    "        \n",
    "        KLD_loss = y_bar * torch.log((y_bar+0.00001)/(y+0.00001)) + (1.0001 - y_bar) * torch.log((1.00001 - y_bar)/(1.00001 - y))\n",
    "        KLD_loss = KLD_loss.mean()\n",
    "\n",
    "        \n",
    "        return KLD_loss\n",
    "    \n",
    "    def bpr_loss(self, users_t, posI_t, negI_t, users_s, posI_s, negI_s):\n",
    "        users_emb_s, pos_emb_s, neg_emb_s, userEmb0_s, pos_Emb0_s = self.getEmbedding_s(users_s.long(), posI_s.long(), negI_s.long())\n",
    "        users_emb_t, pos_emb_t, neg_emb_t, userEmb0_t, pos_Emb0_t = self.getEmbedding_t(users_t.long(), posI_t.long(), negI_t.long())\n",
    "        pred_loss_s = self.pred_loss(users_emb_s, pos_emb_s, neg_emb_s)\n",
    "        pred_loss_t = self.pred_loss(users_emb_t, pos_emb_t, neg_emb_t)\n",
    "        \n",
    "        dp_loss_s = self.dp_loss(posI_s, negI_s, self.embedding_item_s, self.fc_i_t)\n",
    "        dp_loss_t = self.dp_loss(posI_t, negI_t, self.embedding_item_t, self.fc_i_t)\n",
    "          \n",
    "        #---------------------------------------------------------------------------------\n",
    "        #KL_divergence between predictions and knowledge enhanced predictions\n",
    "        \n",
    "        p_s = self.cluster(pos_Emb0_s, self.config[\"tao\"], self.l_clust_encoder_t, self.l_clust_encoder_t2)\n",
    "        p_t = self.cluster(pos_Emb0_t, self.config[\"tao\"], self.l_clust_encoder_t, self.l_clust_encoder_t2) \n",
    "        \n",
    "        p0_s = self.cluster(pos_emb_s, self.config[\"tao\"], self.l_clust_encoder_t, self.l_clust_encoder_t2)\n",
    "        p0_t = self.cluster(pos_emb_t, self.config[\"tao\"], self.l_clust_encoder_t, self.l_clust_encoder_t2)  \n",
    "        \n",
    "        p_s_bar, p_t_bar, _ = self.knowledge_transfer_pp(p_s, p_t, p0_s, p0_t, self.config[\"gamma_kt\"], self.W_pp)   \n",
    "        \n",
    "        pos_emb_bar_t, _, _ = self.knowledge_transfer(pos_emb_t, p_t_bar, self.config[\"gamma_kt\"], self.W_pe_t)\n",
    "        \n",
    "        kld_pos_loss_t = self.kld_loss(users_emb_t, pos_emb_t, pos_emb_bar_t)\n",
    "        kld_loss_t = kld_pos_loss_t \n",
    "        \n",
    "        \n",
    "        #neg_emb_bar_t, _, _ = self.knowledge_transfer(neg_emb_t, p_t_bar, self.config[\"gamma_kt\"], self.W_pe_t)\n",
    "        #kld_neg_loss_t = self.kld_loss(users_emb_t, neg_emb_t, neg_emb_bar_t)\n",
    "        #kld_loss_t = kld_pos_loss_t + kld_neg_loss_t\n",
    "        \n",
    "        #kld_loss_t = torch.Tensor([0.])\n",
    "        \n",
    "        reg_loss = (1/2)*(userEmb0_t.norm(2).pow(2)/float(self.config['bpr_batch_size']*self.latent_dim) + \\\n",
    "                          userEmb0_s.norm(2).pow(2)/float(self.config['bpr_batch_size']*self.latent_dim) + \\\n",
    "                          #p_s.norm(2).pow(2)/float(self.config['n_cluster']*self.latent_dim) + \\\n",
    "                          #p_t.norm(2).pow(2)/float(self.config['n_cluster']*self.latent_dim) + \\\n",
    "                          self.fc_i_t.weight.norm(2).pow(2)/float(768*self.latent_dim) + \\\n",
    "                          self.fc_i_t.bias.norm(2).pow(2)/float(self.latent_dim) + \\\n",
    "                          self.l_clust_encoder_t.weight.norm(2).pow(2)/float(self.latent_dim * self.config[\"n_cluster\"]) + \\\n",
    "                          self.l_clust_encoder_t2.weight.norm(2).pow(2)/float(self.latent_dim * self.config[\"n_cluster\"]) + \\\n",
    "                          self.W_pp.weight.norm(2).pow(2)/float(self.latent_dim * self.latent_dim) + \\\n",
    "                          self.W_pe_t.weight.norm(2).pow(2)/float(self.latent_dim * self.latent_dim)) \n",
    "                            \n",
    "        \n",
    "        return pred_loss_s, pred_loss_t, dp_loss_s, dp_loss_t , kld_loss_t, reg_loss\n",
    "       \n",
    "    def forward(self, users, items, clusters):\n",
    "        # compute embedding\n",
    "        all_users, all_items = self.computer()\n",
    "        # print('forward')\n",
    "        #all_users, all_items = self.computer()\n",
    "        users_emb = all_users[users]\n",
    "        items_emb = all_items[items]\n",
    "        inner_pro = torch.mul(users_emb, items_emb)\n",
    "        gamma     = torch.sum(inner_pro, dim=1)\n",
    "        \n",
    "        # compute embedding\n",
    "        all_users_reg, all_clusters_reg = self.computerClusterReg()\n",
    "        # print('forward')\n",
    "        #all_users, all_items = self.computer()\n",
    "        users_emb_reg = all_users_reg[users]\n",
    "        clusters_emb_reg = all_clusters_reg[clusters]\n",
    "        inner_pro_reg = torch.mul(users_emb_reg, clusters_emb_reg)\n",
    "        gamma_reg     = torch.sum(inner_pro_reg, dim=1)\n",
    "        return gamma, gamma_reg\n",
    "    \n",
    "    def getUsersPartRating_t(self, users, items, all_users, all_items):\n",
    "\n",
    "        users_emb = all_users[users.long()]       \n",
    "        items_emb = all_items[items.long()]\n",
    "        \n",
    "        rating = self.f(torch.mul(users_emb, items_emb).sum(dim=1))\n",
    "        \n",
    "        return rating   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0ede5d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-03T04:06:43.234603Z",
     "start_time": "2022-10-03T04:06:43.230656Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-09T07:35:40.433564Z",
     "iopub.status.busy": "2023-03-09T07:35:40.432975Z",
     "iopub.status.idle": "2023-03-09T07:35:40.438752Z",
     "shell.execute_reply": "2023-03-09T07:35:40.437414Z",
     "shell.execute_reply.started": "2023-03-09T07:35:40.433508Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_tense(x):\n",
    "    return round(float(x),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c2d3a-4802-4f54-8098-d329717e359a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-13T01:51:33.976783Z",
     "iopub.status.busy": "2023-03-13T01:51:33.976187Z",
     "iopub.status.idle": "2023-03-13T01:51:33.981617Z",
     "shell.execute_reply": "2023-03-13T01:51:33.980363Z",
     "shell.execute_reply.started": "2023-03-13T01:51:33.976729Z"
    }
   },
   "source": [
    "# Preprocessing + Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dfc385",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-03T04:58:03.986271Z",
     "start_time": "2022-10-03T04:06:58.055515Z"
    },
    "code_folding": [
     52
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "is_GNN = 1 # lgc-based model: 1, mf-based model:0\n",
    "for Expe_index in [2,3]:\n",
    "    data_path = './Datasets/'\n",
    "\n",
    "    Expe_scenarios = [\"MLtoAM\", \"AMtoML\", \"MLtoAB\", \"ABtoML\", \"AMtoAB\", \"ABtoAM\"]\n",
    "    source_file = [\"Rating_MLasS.csv\", \"Rating_AmzMasS.csv\", \"Rating_MLasS.csv\", \"Rating_AmzBasS.csv\", \"Rating_AmzMasS.csv\",\"Rating_AmzBasS.csv\"]\n",
    "    target_file = [\"Rating_AmzMasT.csv\", \"Rating_MLasT.csv\", \"Rating_AmzBasT.csv\", \"Rating_MLasT.csv\",\"Rating_AmzBasT.csv\", \"Rating_AmzMas.csv\"]\n",
    "\n",
    "\n",
    "    print(\"Experiment scenario: \"+ Expe_scenarios[Expe_index])\n",
    "    print(\"Source domain file: \"+ source_file[Expe_index])\n",
    "    print(\"Target domain file:\" + target_file[Expe_index])\n",
    "\n",
    "    df_S = pd.read_csv(data_path+ source_file[Expe_index])\n",
    "    df_T = pd.read_csv(data_path+ target_file[Expe_index])\n",
    "\n",
    "    if Expe_index>=4:\n",
    "        df_S[\"movieId\"] = df_S.deal_id\n",
    "        df_S[\"userId\"] = df_S.account_id\n",
    "\n",
    "    source_name = Expe_scenarios[Expe_index][:2]\n",
    "    target_name = Expe_scenarios[Expe_index][-2:]\n",
    "\n",
    "    if Expe_index == 1 or Expe_index ==3:\n",
    "        df_S = df_S[[\"account_id\", \"deal_id\"]]\n",
    "        df_S.columns =[\"userId\", \"movieId\"]\n",
    "\n",
    "        df_T = df_T[[\"userId\", \"movieId\"]]\n",
    "        df_T.columns =[\"account_id\", \"deal_id\"]\n",
    "\n",
    "    dict_path = 'Dictionary/'\n",
    "\n",
    "    dict_item_id2index_t = dict(zip(df_T.deal_id.unique(), np.arange(len(df_T.deal_id.unique()))))\n",
    "    dict_user_id2index_t = dict(zip(df_T.account_id.unique(), np.arange(len(df_T.account_id.unique()))))\n",
    "\n",
    "    dict_item_id2index_s = dict(zip(df_S.movieId.unique(), np.arange(len(df_S.movieId.unique()))))\n",
    "    dict_user_id2index_s = dict(zip(df_S.userId.unique(), np.arange(len(df_S.userId.unique()))))\n",
    "\n",
    "    dict_item2vec_s = np.load(dict_path + f'{source_name}to{target_name}/Dict_item2vec_{source_name}asS.npy', allow_pickle=True).item()\n",
    "    vec_matrix = [dict_item2vec_s[deal_id] for deal_id in df_S.movieId.unique()]\n",
    "    dict_ItemIndex2vec_s = dict(zip(np.arange(len(df_S.movieId.unique())), vec_matrix))\n",
    "\n",
    "    dict_item2vec_t = np.load(dict_path + f'{source_name}to{target_name}/Dict_item2vec_{target_name}asT.npy', allow_pickle=True).item()\n",
    "    vec_matrix = [dict_item2vec_t[deal_id] for deal_id in df_T.deal_id.unique()]\n",
    "    dict_ItemIndex2vec_t = dict(zip(np.arange(len(df_T.deal_id.unique())), vec_matrix))\n",
    "\n",
    "    df_T[\"account_index\"] = df_T.account_id.map(lambda x: dict_user_id2index_t[x])\n",
    "    df_T[\"deal_index\"] = df_T.deal_id.map(lambda x: dict_item_id2index_t[x])\n",
    "\n",
    "    df_S[\"account_index\"] = df_S.userId.map(lambda x: dict_user_id2index_s[x])\n",
    "    df_S[\"deal_index\"] = df_S.movieId.map(lambda x: dict_item_id2index_s[x])\n",
    "\n",
    "    def ToList(x):\n",
    "        return list(x)\n",
    "    dict_interactions = dict(df_T.groupby(df_T[\"account_index\"])[\"deal_index\"].apply(ToList))\n",
    "    dict_interactions_s = dict(df_S.groupby(df_S[\"account_index\"])[\"deal_index\"].apply(ToList))\n",
    "\n",
    "    n_users_t = len(df_T.account_index.unique())\n",
    "    m_items_t = len(df_T.deal_index.unique())\n",
    "    n_inters_t = df_T.shape[0]\n",
    "\n",
    "    n_users_s = len(df_S.account_index.unique())\n",
    "    m_items_s = len(df_S.deal_index.unique())\n",
    "    n_inters_s = df_S.shape[0]\n",
    "\n",
    "    print(f\"[Source domain]  n_users:{n_users_s}, m_items:{m_items_s}, n=inter.:{n_inters_s}\")\n",
    "    print(f\"[Target domain]  n_users:{n_users_t}, m_items:{m_items_t}, n=inter.:{n_inters_t}\")\n",
    "\n",
    "    # Build graph\n",
    "\n",
    "    tr_u_s, val_u_s, t_u_s = [], [], []\n",
    "    tr_v_s, val_v_s, t_v_s = [], [], []\n",
    "    for users in dict_interactions_s.keys():\n",
    "        tr_u_s.extend([users] * len(dict_interactions_s[users]))\n",
    "        tr_v_s.extend(dict_interactions_s[users])\n",
    "\n",
    "\n",
    "    tr_u_s= np.array(tr_u_s)\n",
    "    tr_v_s= np.array(tr_v_s)\n",
    "\n",
    "    UseritemNet_s = csr_matrix((np.ones(len(tr_u_s)), (tr_u_s, tr_v_s)), shape=(n_users_s, m_items_s))\n",
    "\n",
    "    tr_u_t, val_u_t, t_u_t = [], [], []\n",
    "    tr_v_t, val_v_t, t_v_t = [], [], []\n",
    "    for users in dict_interactions.keys():\n",
    "        tr_u_t.extend([users] * len(dict_interactions[users][:-2]))\n",
    "        val_u_t.append(users)\n",
    "        t_u_t.append(users)\n",
    "\n",
    "        tr_v_t.extend(dict_interactions[users][:-2])\n",
    "        val_v_t.append(dict_interactions[users][-2])\n",
    "        t_v_t.append(dict_interactions[users][-1])\n",
    "\n",
    "    tr_u_t, val_u_t, t_u_t = np.array(tr_u_t), np.array(val_u_t), np.array(t_u_t)\n",
    "    tr_v_t, val_v_t, t_v_t = np.array(tr_v_t), np.array(val_v_t), np.array(t_v_t)\n",
    "\n",
    "    UseritemNet_t = csr_matrix((np.ones(len(tr_u_t)), (tr_u_t, tr_v_t)), shape=(n_users_t, m_items_t))\n",
    "\n",
    "\n",
    "\n",
    "    config={}\n",
    "    config['bpr_batch_size'] = 2048\n",
    "    config['latent_dim_rec'] = 32\n",
    "    config['lightGCN_n_layers_t']= 3\n",
    "    config['lightGCN_n_layers_s']= 3\n",
    "    config['test_u_batch_size'] = 100\n",
    "    config['lr'] = 0.01\n",
    "    config['decay_reg'] = 0.01 # beta Regularization loss\n",
    "    config['decay_dp'] = 1 # gamma Reconstruction loss\n",
    "    config['decay_kt'] = 0.1 #alpha Restriction loss\n",
    "    config[\"seed\"] = 6298\n",
    "    config[\"epochs\"] = 2000\n",
    "    config[\"n_cluster\"] = 32\n",
    "    config[\"tao\"] = 0.0001\n",
    "    config[\"gamma_kt\"] = 0.1\n",
    "    #----------------------------------\n",
    "    config['dropout'] = 0\n",
    "    config['keep_prob']  = 0.6\n",
    "    config['A_n_fold'] = 100\n",
    "    config['multicore'] = 0\n",
    "    config[\"topks\"] = [20]\n",
    "\n",
    "    UIGraph_s = getSparseGraph(n_users_s, m_items_s, UseritemNet_s)\n",
    "    UIGraph_t = getSparseGraph(n_users_t, m_items_t, UseritemNet_t)\n",
    "    if is_GNN:\n",
    "        weight_file = f\"{Expe_scenarios[Expe_index]}-SRTrans-wo_dp-LGC-Reg_{config['decay_reg']}-Rec_{config['decay_dp']}-Kt_{config['decay_kt']}-{config['latent_dim_rec']}.pth.tar\"\n",
    "    else:\n",
    "        weight_file = f\"{Expe_scenarios[Expe_index]}-SRTrans-wo_dp-MF-Reg_{config['decay_reg']}-Rec_{config['decay_dp']}-Kt_{config['decay_kt']}-{config['latent_dim_rec']}.pth.tar\"\n",
    "    Recmodel = SRTrans(config,UIGraph_s, UIGraph_t, is_GNN)\n",
    "    #Recmodel = SRTrans(config)\n",
    "    patient = 8\n",
    "    min_ = 0.\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "\n",
    "        if epoch %10 == 0 and epoch!=0:\n",
    "            print(\"\")\n",
    "            results = Test(1)\n",
    "            print(\"[TEST] recall:{0}, ndcg:{1}\".format(results['recall'][0], results['ndcg'][0]))\n",
    "\n",
    "            if results['ndcg'][0] > min_:\n",
    "                torch.save(Recmodel.state_dict(), weight_file)\n",
    "                min_ = results['ndcg'][0]\n",
    "                patient = 8\n",
    "                continue\n",
    "\n",
    "            if results['ndcg'][0] <= min_:\n",
    "                patient = patient - 1\n",
    "\n",
    "            if patient == 0:                      \n",
    "                break\n",
    "\n",
    "        start = time.time()\n",
    "        aver_loss, aver_pre_loss_s, aver_pre_loss_t, aver_rec_loss_s, aver_rec_loss_t, aver_kt_loss, aver_reg_loss = BPR_train_original(epoch)\n",
    "        end = time.time()\n",
    "        sys.stdout.write(\"\\r ||epoch:{0}||loss:{1}||pre_loss_s:{2}||pre_loss_t:{3}||rec_loss_s:{4}||rec_loss_t:{5}||kt_loss:{6}||reg_loss:{7}||time:{8}\".format(\n",
    "            epoch, print_tense(aver_loss), print_tense(aver_pre_loss_s), print_tense(aver_pre_loss_t), \n",
    "            print_tense(aver_rec_loss_s), print_tense(aver_rec_loss_t), print_tense(aver_kt_loss), print_tense(aver_reg_loss), print_tense(end-start)))\n",
    "        sys.stdout.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c8b3b-a91e-4d75-a8a4-48d867366682",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20420975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-03T05:09:35.824758Z",
     "start_time": "2022-10-03T05:09:35.810747Z"
    },
    "code_folding": [
     0,
     13,
     16
    ],
    "execution": {
     "iopub.execute_input": "2023-03-09T07:20:09.640446Z",
     "iopub.status.busy": "2023-03-09T07:20:09.639867Z",
     "iopub.status.idle": "2023-03-09T07:20:09.655705Z",
     "shell.execute_reply": "2023-03-09T07:20:09.654720Z",
     "shell.execute_reply.started": "2023-03-09T07:20:09.640393Z"
    }
   },
   "outputs": [],
   "source": [
    "def Evaluation(Reclist, groundtruth, t_u, K):\n",
    "    hit_rite = []\n",
    "    ndcg = []\n",
    "    for i in K:\n",
    "        hr_i = 0.\n",
    "        ndcg_i = 0.\n",
    "        for j in range(len(t_u)):\n",
    "            hr_i += len(set(Reclist[t_u[j]][:i+1]) & set([groundtruth[j]]))\n",
    "            ndcg_i += getNDCG(Reclist[t_u[j]][:i+1], [groundtruth[j]])\n",
    "        hit_rite.append(hr_i / len(groundtruth))\n",
    "        ndcg.append(ndcg_i / len(groundtruth))\n",
    "    return hit_rite, ndcg\n",
    "def getDCG(scores):\n",
    "    return np.sum(\n",
    "        np.divide(np.power(2, scores) - 1, np.log(np.arange(scores.shape[0], dtype=np.float32) + 2)),\n",
    "        dtype=np.float32)\n",
    "def getNDCG(rank_list, pos_items):\n",
    "    relevance = np.ones_like(pos_items)\n",
    "    it2rel = {it: r for it, r in zip(pos_items, relevance)}\n",
    "    rank_scores = np.asarray([it2rel.get(it, 0.0) for it in rank_list], dtype=np.float32)\n",
    "\n",
    "    idcg = getDCG(relevance)\n",
    "\n",
    "    dcg = getDCG(rank_scores)\n",
    "\n",
    "    if dcg == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5982dfbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-03T05:09:36.491324Z",
     "start_time": "2022-10-03T05:09:36.482212Z"
    },
    "code_folding": [
     1,
     8
    ],
    "execution": {
     "iopub.execute_input": "2023-03-09T07:20:10.582117Z",
     "iopub.status.busy": "2023-03-09T07:20:10.581528Z",
     "iopub.status.idle": "2023-03-09T07:20:11.276047Z",
     "shell.execute_reply": "2023-03-09T07:20:11.275271Z",
     "shell.execute_reply.started": "2023-03-09T07:20:10.582061Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, h\n",
    "\n",
    "def output_result(data):\n",
    "    data = np.array(data).T\n",
    "    for i in range(len(data)):\n",
    "        m, h = mean_confidence_interval(data[i], confidence=0.95)\n",
    "        m = round(m,3)\n",
    "        h = round(h,3)\n",
    "        print(f\"{m} Â± {h}  \", end=\" \")\n",
    "        #print(m, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339315bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-03T05:17:33.684276Z",
     "start_time": "2022-10-03T05:10:05.721445Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for Expe_index in [2,3]:\n",
    "    work_path = '/home/lizhi/Experiment_4_CIKM/'\n",
    "    data_path = work_path + 'Datasets/'\n",
    "\n",
    "    Expe_scenarios = [\"MLtoAM\", \"AMtoML\", \"MLtoAB\", \"ABtoML\", \"AMtoAB\", \"ABtoAM\"]\n",
    "    source_file = [\"Rating_MLasS.csv\", \"Rating_AmzMasS.csv\", \"Rating_MLasS.csv\", \"Rating_AmzBasS.csv\", \"Rating_AmzMasS.csv\",\"Rating_AmzBasS.csv\"]\n",
    "    target_file = [\"Rating_AmzMasT.csv\", \"Rating_MLasT.csv\", \"Rating_AmzBasT.csv\", \"Rating_MLasT.csv\",\"Rating_AmzBasT.csv\", \"Rating_AmzMas.csv\"]\n",
    "\n",
    "\n",
    "    print(\"Experiment scenario: \"+ Expe_scenarios[Expe_index])\n",
    "    print(\"Source domain file: \"+ source_file[Expe_index])\n",
    "    print(\"Target domain file:\" + target_file[Expe_index])\n",
    "\n",
    "    df_S = pd.read_csv(data_path+ source_file[Expe_index])\n",
    "    df_T = pd.read_csv(data_path+ target_file[Expe_index])\n",
    "\n",
    "    if Expe_index>=4:\n",
    "        df_S[\"movieId\"] = df_S.deal_id\n",
    "        df_S[\"userId\"] = df_S.account_id\n",
    "\n",
    "    source_name = Expe_scenarios[Expe_index][:2]\n",
    "    target_name = Expe_scenarios[Expe_index][-2:]\n",
    "\n",
    "    if Expe_index == 1 or Expe_index ==3:\n",
    "        df_S = df_S[[\"account_id\", \"deal_id\"]]\n",
    "        df_S.columns =[\"userId\", \"movieId\"]\n",
    "\n",
    "        df_T = df_T[[\"userId\", \"movieId\"]]\n",
    "        df_T.columns =[\"account_id\", \"deal_id\"]\n",
    "\n",
    "    dict_path = work_path + 'Dictionary/'\n",
    "\n",
    "    dict_item_id2index_t = dict(zip(df_T.deal_id.unique(), np.arange(len(df_T.deal_id.unique()))))\n",
    "    dict_user_id2index_t = dict(zip(df_T.account_id.unique(), np.arange(len(df_T.account_id.unique()))))\n",
    "\n",
    "    dict_item_id2index_s = dict(zip(df_S.movieId.unique(), np.arange(len(df_S.movieId.unique()))))\n",
    "    dict_user_id2index_s = dict(zip(df_S.userId.unique(), np.arange(len(df_S.userId.unique()))))\n",
    "\n",
    "    dict_item2vec_s = np.load(dict_path + f'{source_name}to{target_name}/Dict_item2vec_{source_name}asS.npy', allow_pickle=True).item()\n",
    "    vec_matrix = [dict_item2vec_s[deal_id] for deal_id in df_S.movieId.unique()]\n",
    "    dict_ItemIndex2vec_s = dict(zip(np.arange(len(df_S.movieId.unique())), vec_matrix))\n",
    "\n",
    "    dict_item2vec_t = np.load(dict_path + f'{source_name}to{target_name}/Dict_item2vec_{target_name}asT.npy', allow_pickle=True).item()\n",
    "    vec_matrix = [dict_item2vec_t[deal_id] for deal_id in df_T.deal_id.unique()]\n",
    "    dict_ItemIndex2vec_t = dict(zip(np.arange(len(df_T.deal_id.unique())), vec_matrix))\n",
    "\n",
    "    df_T[\"account_index\"] = df_T.account_id.map(lambda x: dict_user_id2index_t[x])\n",
    "    df_T[\"deal_index\"] = df_T.deal_id.map(lambda x: dict_item_id2index_t[x])\n",
    "\n",
    "    df_S[\"account_index\"] = df_S.userId.map(lambda x: dict_user_id2index_s[x])\n",
    "    df_S[\"deal_index\"] = df_S.movieId.map(lambda x: dict_item_id2index_s[x])\n",
    "\n",
    "    def ToList(x):\n",
    "        return list(x)\n",
    "    dict_interactions = dict(df_T.groupby(df_T[\"account_index\"])[\"deal_index\"].apply(ToList))\n",
    "    dict_interactions_s = dict(df_S.groupby(df_S[\"account_index\"])[\"deal_index\"].apply(ToList))\n",
    "\n",
    "    n_users_t = len(df_T.account_index.unique())\n",
    "    m_items_t = len(df_T.deal_index.unique())\n",
    "    n_inters_t = df_T.shape[0]\n",
    "\n",
    "    n_users_s = len(df_S.account_index.unique())\n",
    "    m_items_s = len(df_S.deal_index.unique())\n",
    "    n_inters_s = df_S.shape[0]\n",
    "\n",
    "    print(f\"[Source domain]  n_users:{n_users_s}, m_items:{m_items_s}, n=inter.:{n_inters_s}\")\n",
    "    print(f\"[Target domain]  n_users:{n_users_t}, m_items:{m_items_t}, n=inter.:{n_inters_t}\")\n",
    "\n",
    "    # Build graph\n",
    "\n",
    "    tr_u_s, val_u_s, t_u_s = [], [], []\n",
    "    tr_v_s, val_v_s, t_v_s = [], [], []\n",
    "    for users in dict_interactions_s.keys():\n",
    "        tr_u_s.extend([users] * len(dict_interactions_s[users]))\n",
    "        tr_v_s.extend(dict_interactions_s[users])\n",
    "\n",
    "\n",
    "    tr_u_s= np.array(tr_u_s)\n",
    "    tr_v_s= np.array(tr_v_s)\n",
    "\n",
    "    UseritemNet_s = csr_matrix((np.ones(len(tr_u_s)), (tr_u_s, tr_v_s)), shape=(n_users_s, m_items_s))\n",
    "\n",
    "    tr_u_t, val_u_t, t_u_t = [], [], []\n",
    "    tr_v_t, val_v_t, t_v_t = [], [], []\n",
    "    for users in dict_interactions.keys():\n",
    "        tr_u_t.extend([users] * len(dict_interactions[users][:-2]))\n",
    "        val_u_t.append(users)\n",
    "        t_u_t.append(users)\n",
    "\n",
    "        tr_v_t.extend(dict_interactions[users][:-2])\n",
    "        val_v_t.append(dict_interactions[users][-2])\n",
    "        t_v_t.append(dict_interactions[users][-1])\n",
    "\n",
    "    tr_u_t, val_u_t, t_u_t = np.array(tr_u_t), np.array(val_u_t), np.array(t_u_t)\n",
    "    tr_v_t, val_v_t, t_v_t = np.array(tr_v_t), np.array(val_v_t), np.array(t_v_t)\n",
    "\n",
    "    UseritemNet_t = csr_matrix((np.ones(len(tr_u_t)), (tr_u_t, tr_v_t)), shape=(n_users_t, m_items_t))\n",
    "    \n",
    "    UIGraph_s = getSparseGraph(n_users_s, m_items_s, UseritemNet_s)\n",
    "    UIGraph_t = getSparseGraph(n_users_t, m_items_t, UseritemNet_t)\n",
    "    \n",
    "    item_list = df_T.deal_index.unique()\n",
    "    \n",
    "\n",
    "    if is_GNN:\n",
    "        weight_file = f\"{Expe_scenarios[Expe_index]}-SRTrans-wo_dp-LGC-Reg_{config['decay_reg']}-Rec_{config['decay_dp']}-Kt_{config['decay_kt']}-{config['latent_dim_rec']}.pth.tar\"\n",
    "    else:\n",
    "        weight_file = f\"{Expe_scenarios[Expe_index]}-SRTrans-wo_dp-MF-Reg_{config['decay_reg']}-Rec_{config['decay_dp']}-Kt_{config['decay_kt']}-{config['latent_dim_rec']}.pth.tar\"\n",
    "    \n",
    "    model = SRTrans(config, UIGraph_s, UIGraph_t, is_GNN)\n",
    "    model.load_state_dict(torch.load(weight_file)) \n",
    "    model.eval()\n",
    "\n",
    "    HR = []\n",
    "    NDCG = []\n",
    "    \n",
    "    all_users, all_items = model.computer_t()\n",
    "    for i in range(10):\n",
    "        np.random.seed(i*7) \n",
    "        condidate_item = np.random.choice(item_list, 120, replace=False)\n",
    "\n",
    "        pred_reclist = {}\n",
    "        item_matrix = []\n",
    "        for index in tqdm(range(len(t_u_t))):\n",
    "            u = t_u_t[index]\n",
    "            u_condidate_item = np.setdiff1d(condidate_item, dict_interactions[u])\n",
    "            u_condidate_item = np.random.choice(u_condidate_item, 99, replace=False)\n",
    "            u_condidate_item = np.union1d(u_condidate_item, dict_interactions[u][-1:])\n",
    "            item_matrix.append(u_condidate_item)\n",
    "        v = torch.LongTensor(item_matrix)\n",
    "        v = v.t()\n",
    "        pred_result = []\n",
    "\n",
    "        for j in tqdm(range(100)):\n",
    "            y = model.getUsersPartRating_t(torch.LongTensor(t_u_t).cuda(), v[j].cuda(), all_users, all_items).cpu()\n",
    "            pred_result.append(list(y.data))\n",
    "\n",
    "        result = torch.Tensor(pred_result).t()\n",
    "        #result = pred_result.t()\n",
    "        topk_indices = np.array(torch.topk(result, 100, dim=1).indices)\n",
    "\n",
    "        topk_items = []\n",
    "        item_matrix = np.array(item_matrix)\n",
    "        for i in range(len(topk_indices)):\n",
    "            topk_items.append(item_matrix[i][topk_indices[i]])\n",
    "        topk_items = np.array(topk_items)\n",
    "\n",
    "        pred_reclist = dict(zip(t_u_t, topk_items))\n",
    "\n",
    "        hit_rite, ndcg = Evaluation(pred_reclist, t_v_t, t_u_t, [i for i in range(10)])\n",
    "        HR.append(hit_rite)\n",
    "        NDCG.append(ndcg)\n",
    "    output_result(HR)\n",
    "    print()\n",
    "    output_result(NDCG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_1.1.0",
   "language": "python",
   "name": "torch_1.1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "199.576px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
