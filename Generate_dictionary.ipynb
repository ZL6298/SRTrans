{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T07:05:26.388122Z",
     "start_time": "2022-09-01T07:05:24.986570Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "import mojimoji\n",
    "import re\n",
    "import collections\n",
    "import torchtext\n",
    "from tqdm import tqdm\n",
    "from torchtext.data import Field, Dataset, Example\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T07:05:29.201286Z",
     "start_time": "2022-09-01T07:05:29.196722Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda = torch.device('cuda') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T07:05:32.227701Z",
     "start_time": "2022-09-01T07:05:32.195620Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def deal_long_document(inputs, index, net_1, tfidf_weight):\n",
    "    doc_vec = torch.zeros(768)\n",
    "    for x in range(inputs[\"input_ids\"].shape[0] // 512 + 1):\n",
    "        input_sub = {}\n",
    "        try:\n",
    "            input_sub[\"input_ids\"] = inputs[\"input_ids\"][:,x*512:(x+1)*512]\n",
    "            input_sub[\"token_type_ids\"] = inputs[\"token_type_ids\"][:,x*512:(x+1)*512] \n",
    "            input_sub[\"attention_mask\"] = inputs[\"attention_mask\"][:,x*512:(x+1)*512]\n",
    "            outputs = net_1(**input_sub)\n",
    "            vec = torch.Tensor(tfidf_weight[index][x*512:(x+1)*512]).matmul(outputs.hidden_states[0][0]).data\n",
    "        except:\n",
    "            input_sub[\"input_ids\"] = inputs[\"input_ids\"][:,x*512:]\n",
    "            input_sub[\"token_type_ids\"] = inputs[\"token_type_ids\"][:,x*512:] \n",
    "            input_sub[\"attention_mask\"] = inputs[\"attention_mask\"][:,x*512:]  \n",
    "            outputs = net_1(**input_sub)\n",
    "            vec = torch.Tensor(tfidf_weight[index][x*512:]).matmul(outputs.hidden_states[0][0]).data\n",
    "        doc_vec += vec\n",
    "    return doc_vec\n",
    "\n",
    "def Calculate_item_embedding(source, target):\n",
    "    # Download the pretrained BERT model.\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    net_1 = transformers.BertForPreTraining.from_pretrained('bert-base-uncased', output_hidden_states=True)    \n",
    "    net_1.eval()\n",
    "    if source == \"ML\":\n",
    "        df_movies = pd.read_csv(\"./Datasets/Side_MLasS.csv\")\n",
    "    if source == \"AM\":\n",
    "        df_AmazonMovie_sub = pd.read_csv(\"./Datasets/Side_AmzMasS.csv\")\n",
    "    if source == \"AB\":\n",
    "        df_AmazonMovie_sub = pd.read_csv(\"./Datasets/Side_AmzBasS.csv\")     \n",
    "        \n",
    "    if target == \"ML\":\n",
    "        df_movies = pd.read_csv(\"./Datasets/Side_MLasT.csv\")\n",
    "    if target == \"AM\":\n",
    "        df_AmazonMovie_sub = pd.read_csv(\"./Datasets/Side_AmzMasT.csv\")\n",
    "    if target == \"AB\":\n",
    "        df_AmazonMovie_sub = pd.read_csv(\"./Datasets/Side_AmzBasT.csv\")         \n",
    "        \n",
    "    dict_id2descripton = np.load(\"/home/lizhi/CDRS-GNN/\" + \"Dict_MovieId2description.npy\", allow_pickle=True).item()\n",
    "    df_movies[\"description\"] = df_movies.movieId.map(lambda x: dict_id2descripton[x])\n",
    "    df_movies[\"tokenized_word_id\"] = df_movies.description.map(lambda x: tokenizer(x, return_tensors=\"pt\")[\"input_ids\"].tolist())\n",
    "    \n",
    "    df_AmazonMovie_sub[\"description\"] = df_AmazonMovie_sub.description.map(lambda x: str(x))\n",
    "    df_AmazonMovie_sub[\"tokenized_word_id\"] = df_AmazonMovie_sub.description.map(lambda x: tokenizer(str(x), return_tensors=\"pt\")[\"input_ids\"].tolist())\n",
    "    \n",
    "    tfidf_weight = CalculateTFidf(df_movies, df_AmazonMovie_sub)\n",
    "    \n",
    "    document = df_movies.description.values\n",
    "    document2 = df_AmazonMovie_sub.description.values\n",
    "    documents = np.concatenate((document, document2), axis=0)\n",
    "\n",
    "    vec_list = []\n",
    "    for i in tqdm(range(len(documents))):\n",
    "        inputs = tokenizer(documents[i], return_tensors=\"pt\")\n",
    "        if inputs[\"input_ids\"].shape[1] <= 512: \n",
    "            outputs = net_1(**inputs)\n",
    "            doc_vec = torch.Tensor(tfidf_weight[i]).matmul(outputs.hidden_states[0][0]).data\n",
    "        else:\n",
    "            doc_vec = deal_long_document(inputs, i, net_1, tfidf_weight)\n",
    "        vec_list.append(list(doc_vec.data / inputs[\"input_ids\"].shape[1]))\n",
    "\n",
    "    vec_list_2 = []\n",
    "    for vector in vec_list:\n",
    "        vec_list_2.append([float(value) for value in vector])\n",
    "\n",
    "    dict_MLMovie2vec = dict(zip(df_movies.movieId.values, vec_list_2[:len(df_movies.movieId.values)]))\n",
    "    dict_AmazonMovie2vec = dict(zip(df_AmazonMovie_sub.deal_id.values, vec_list_2[len(df_movies.movieId.values):]))\n",
    "    \n",
    "    #np.save(\"Dict_item2vec_A.npy\", dict_AmazonMovie2vec)\n",
    "    #np.save(\"Dict_item2vec_M.npy\", dict_MLMovie2vec)   \n",
    "    if source == \"ML\":\n",
    "        return dict_MLMovie2vec, dict_AmazonMovie2vec\n",
    "    if target == \"ML\":\n",
    "        return dict_AmazonMovie2vec, dict_MLMovie2vec\n",
    "    else:\n",
    "        print(\"Error: No ML dataset.\")\n",
    "        return 0., 0.\n",
    "\n",
    "def CalculateTFidf(df_movies, df_AmazonMovie_sub):\n",
    "    dataset = df_movies.tokenized_word_id.values\n",
    "    dataset2 = df_AmazonMovie_sub.tokenized_word_id.values\n",
    "    data = np.concatenate((dataset, dataset2), axis=0)\n",
    "\n",
    "    data_str = list(map(lambda x:x[0],data))\n",
    "    for i in range(len(data_str)):\n",
    "        for j in range(len(data_str[i])):\n",
    "            data_str[i][j] = str(data_str[i][j])\n",
    "    dictionary = Dictionary(data_str)\n",
    "    corpus = list(map(dictionary.doc2bow,data_str))\n",
    "    model = TfidfModel(corpus)\n",
    "    corpus_tfidf = model[corpus]\n",
    "    tfidf_weight = []\n",
    "    for i in tqdm(range(len(data_str))):\n",
    "        tfidf_vec = []\n",
    "        dict_id_2_tfidf = dict(zip([x[0] for x in corpus_tfidf[i]], [x[1] for x in corpus_tfidf[i]]))\n",
    "        for token in dictionary.doc2idx(data_str[i]):\n",
    "            try:\n",
    "                tfidf_vec.append(dict_id_2_tfidf[token])\n",
    "            except:\n",
    "                tfidf_vec.append(0.)\n",
    "        tfidf_weight.append(tfidf_vec)\n",
    "    return tfidf_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T07:59:04.731789Z",
     "start_time": "2022-09-01T07:59:04.719373Z"
    }
   },
   "outputs": [],
   "source": [
    "def deal_long_document_lastlayer(inputs, index, net_1):\n",
    "    doc_vec = torch.zeros(768).cuda()\n",
    "    for x in range(inputs[\"input_ids\"].shape[0] // 512 + 1):\n",
    "        input_sub = {}\n",
    "        try:\n",
    "            input_sub[\"input_ids\"] = inputs[\"input_ids\"][:,x*512:(x+1)*512]\n",
    "            input_sub[\"token_type_ids\"] = inputs[\"token_type_ids\"][:,x*512:(x+1)*512] \n",
    "            input_sub[\"attention_mask\"] = inputs[\"attention_mask\"][:,x*512:(x+1)*512]\n",
    "            vec = net_1(**input_sub).hidden_states[-1][0].mean(dim=0).data\n",
    "        except:\n",
    "            input_sub[\"input_ids\"] = inputs[\"input_ids\"][:,x*512:]\n",
    "            input_sub[\"token_type_ids\"] = inputs[\"token_type_ids\"][:,x*512:] \n",
    "            input_sub[\"attention_mask\"] = inputs[\"attention_mask\"][:,x*512:]  \n",
    "            vec = net_1(**input_sub).hidden_states[-1][0].mean(dim=0).data\n",
    "        doc_vec += vec\n",
    "    return doc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T07:28:17.481637Z",
     "start_time": "2022-09-01T07:28:17.430380Z"
    }
   },
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=1\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T07:59:40.141811Z",
     "start_time": "2022-09-01T07:59:40.119259Z"
    }
   },
   "outputs": [],
   "source": [
    "def Calculate_item_embedding_lastlayer(source, target):\n",
    "    # Download the pretrained BERT model.\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    net_1 = transformers.BertForPreTraining.from_pretrained('bert-base-uncased', output_hidden_states=True)    \n",
    "    net_1.eval()\n",
    "    net_1.cuda()\n",
    "    if source == \"ML\":\n",
    "        df_movies = pd.read_csv(\"./Datasets/Side_MLasS.csv\")\n",
    "    if source == \"AM\":\n",
    "        df_AmazonMovie_sub = pd.read_csv(\"./Datasets/Side_AmzMasS.csv\")\n",
    "    if source == \"AB\":\n",
    "        df_AmazonMovie_sub = pd.read_csv(\"./Datasets/Side_AmzBasS.csv\")     \n",
    "        \n",
    "    if target == \"ML\":\n",
    "        df_movies = pd.read_csv(\"./Datasets/Side_MLasT.csv\")\n",
    "    if target == \"AM\":\n",
    "        df_AmazonMovie_sub = pd.read_csv(\"./Datasets/Side_AmzMasT.csv\")\n",
    "    if target == \"AB\":\n",
    "        df_AmazonMovie_sub = pd.read_csv(\"./Datasets/Side_AmzBasT.csv\")         \n",
    "        \n",
    "    dict_id2descripton = np.load(\"/home/lizhi/CDRS-GNN/\" + \"Dict_MovieId2description.npy\", allow_pickle=True).item()\n",
    "    df_movies[\"description\"] = df_movies.movieId.map(lambda x: dict_id2descripton[x])\n",
    "    #df_movies[\"tokenized_word_id\"] = df_movies.description.map(lambda x: tokenizer(x, return_tensors=\"pt\")[\"input_ids\"].tolist())\n",
    "    \n",
    "    df_AmazonMovie_sub[\"description\"] = df_AmazonMovie_sub.description.map(lambda x: str(x))\n",
    "    #df_AmazonMovie_sub[\"tokenized_word_id\"] = df_AmazonMovie_sub.description.map(lambda x: tokenizer(str(x), return_tensors=\"pt\")[\"input_ids\"].tolist())\n",
    "    \n",
    "    #tfidf_weight = CalculateTFidf(df_movies, df_AmazonMovie_sub)\n",
    "    \n",
    "    document = df_movies.description.values\n",
    "    document2 = df_AmazonMovie_sub.description.values\n",
    "    documents = np.concatenate((document, document2), axis=0)\n",
    "\n",
    "    vec_list = []\n",
    "    '''\n",
    "    for i in tqdm(range(len(documents))):\n",
    "        inputs = tokenizer(documents[i], return_tensors=\"pt\")\n",
    "        if inputs[\"input_ids\"].shape[1] <= 512: \n",
    "            outputs = net_1(**inputs)\n",
    "            doc_vec = torch.Tensor(tfidf_weight[i]).matmul(outputs.hidden_states[-1][0]).data\n",
    "        else:\n",
    "            doc_vec = deal_long_document(inputs, i, net_1, tfidf_weight)\n",
    "        vec_list.append(list(doc_vec.data / inputs[\"input_ids\"].shape[1]))\n",
    "    '''\n",
    "    \n",
    "    for i in tqdm(range(len(documents))):\n",
    "        sentences = documents[i].split(\".\")\n",
    "        doc_vec = torch.zeros(768).cuda()\n",
    "        for j in range(len(sentences)):\n",
    "            if len(sentences[j])<5:\n",
    "                continue\n",
    "            inputs = tokenizer(sentences[j], return_tensors=\"pt\")\n",
    "            inputs[\"input_ids\"] = inputs[\"input_ids\"].cuda()\n",
    "            inputs[\"token_type_ids\"] = inputs[\"token_type_ids\"].cuda()\n",
    "            inputs[\"attention_mask\"] = inputs[\"attention_mask\"].cuda()\n",
    "            if len(sentences[j])<=512:\n",
    "                outputs = net_1(**inputs).hidden_states[-1][0].mean(dim=0).data\n",
    "            else:\n",
    "                outputs = deal_long_document_lastlayer(inputs, i, net_1)\n",
    "           \n",
    "            doc_vec += outputs\n",
    "            \n",
    "        vec_list.append(list(doc_vec.cpu() / len(sentences)))\n",
    "    \n",
    "    vec_list_2 = []\n",
    "    for vector in vec_list:\n",
    "        vec_list_2.append([float(value) for value in vector])\n",
    "\n",
    "    dict_MLMovie2vec = dict(zip(df_movies.movieId.values, vec_list_2[:len(df_movies.movieId.values)]))\n",
    "    dict_AmazonMovie2vec = dict(zip(df_AmazonMovie_sub.deal_id.values, vec_list_2[len(df_movies.movieId.values):]))\n",
    "    \n",
    "    #np.save(\"Dict_item2vec_A.npy\", dict_AmazonMovie2vec)\n",
    "    #np.save(\"Dict_item2vec_M.npy\", dict_MLMovie2vec)   \n",
    "    if source == \"ML\":\n",
    "        return dict_MLMovie2vec, dict_AmazonMovie2vec\n",
    "    if target == \"ML\":\n",
    "        return dict_AmazonMovie2vec, dict_MLMovie2vec\n",
    "    else:\n",
    "        print(\"Error: No ML dataset.\")\n",
    "        return 0., 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T07:05:32.863839Z",
     "start_time": "2022-09-01T07:05:32.845767Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def ItemClustering(dict_itemId2vec_A, dict_itemId2vec_M, num_clusters):\n",
    "\n",
    "    dict_itemindex2vec_M = dict(zip(np.arange(len(dict_itemId2vec_M)), dict_itemId2vec_M.values()))\n",
    "    dict_itemindex2vec_A = dict(zip(np.arange(len(dict_itemId2vec_A)), dict_itemId2vec_A.values()))\n",
    "\n",
    "    dict_itemindex2ID_M = dict(zip(np.arange(len(dict_itemId2vec_M)), dict_itemId2vec_M.keys()))\n",
    "    dict_itemindex2ID_A = dict(zip(np.arange(len(dict_itemId2vec_A)), dict_itemId2vec_A.keys()))\n",
    "\n",
    "    dict_M_sample = {}\n",
    "    for item in range(len(dict_itemId2vec_M)):\n",
    "        dict_M_sample[item] = dict_itemindex2vec_M[item]\n",
    "    dict_A_sample = {}\n",
    "    for item in range(len(dict_itemId2vec_A)):\n",
    "        dict_A_sample[item] = dict_itemindex2vec_A[item]    \n",
    "\n",
    "    Jointed_items = list(dict_M_sample.values())\n",
    "    Jointed_items.extend(dict_A_sample.values())   \n",
    "    jointed_norm = preprocessing.normalize(Jointed_items, norm='l2')\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=6298).fit(jointed_norm)\n",
    "    lables = kmeans.predict(jointed_norm)\n",
    "\n",
    "    #gmm = GaussianMixture(n_components=num_clusters).fit(jointed_norm)\n",
    "    #lables = gmm.predict(jointed_norm)\n",
    "\n",
    "    Cluster_M = dict(pd.value_counts(lables[:len(dict_M_sample)]))\n",
    "    Cluster_A = dict(pd.value_counts(lables[len(dict_M_sample):]))\n",
    "\n",
    "    dict_itemId2Cluster_A = dict(zip(dict_itemId2vec_A.keys(), lables[len(dict_M_sample):]))\n",
    "    dict_itemId2Cluster_M = dict(zip(dict_itemId2vec_M.keys(), lables[:len(dict_M_sample)]))\n",
    "\n",
    "    #np.save(\"Dict_itemId2Cluster_T.npy\", dict_itemId2Cluster_A)\n",
    "    #np.save(\"Dict_itemId2Cluster_S.npy\", dict_itemId2Cluster_M)\n",
    "    \n",
    "    \n",
    "    count = np.zeros([num_clusters])\n",
    "    cluster_matrix = np.zeros([num_clusters,768])\n",
    "    for key, values in zip(dict_itemId2Cluster_A.keys(), dict_itemId2Cluster_A.values()):\n",
    "        cluster_matrix[values] += dict_itemId2vec_A[key]\n",
    "        count[values] += 1\n",
    "    for key, values in zip(dict_itemId2Cluster_M.keys(), dict_itemId2Cluster_M.values()):\n",
    "        cluster_matrix[values] += dict_itemId2vec_M[key]\n",
    "        count[values] += 1\n",
    "    \n",
    "    for i in range(num_clusters):\n",
    "        cluster_matrix[i] = cluster_matrix[i] / count[i]\n",
    "    dict_clusterId2vec = dict(zip(np.arange(num_clusters), cluster_matrix))\n",
    "    #np.save(\"Dict_clusterId2vec.npy\", dict_clusterId2vec)\n",
    "    \n",
    "    return dict_itemId2Cluster_A, dict_itemId2Cluster_M, dict_clusterId2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T07:05:33.806364Z",
     "start_time": "2022-09-01T07:05:33.786785Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def Calculate_item_embedding_2(source, target):\n",
    "    # Download the pretrained BERT model.\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    net_1 = transformers.BertForPreTraining.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "    net_1.eval()\n",
    "    if source == \"AM\":\n",
    "        df_movies = pd.read_csv(\"./Datasets/Side_AmzMasS.csv\")\n",
    "    if source == \"AB\":\n",
    "        df_movies = pd.read_csv(\"./Datasets/Side_AmzBasS.csv\")     \n",
    "        \n",
    "    if target == \"AM\":\n",
    "        df_AmazonMovie_sub = pd.read_csv(\"./Datasets/Side_AmzMasT.csv\")\n",
    "    if target == \"AB\":\n",
    "        df_AmazonMovie_sub = pd.read_csv(\"./Datasets/Side_AmzBasT.csv\")         \n",
    "        \n",
    "    dict_id2descripton = np.load(\"/home/lizhi/CDRS-GNN/\" + \"Dict_MovieId2description.npy\", allow_pickle=True).item()\n",
    "    df_movies[\"description\"] = df_movies.description.map(lambda x: str(x))\n",
    "    df_movies[\"tokenized_word_id\"] = df_movies.description.map(lambda x: tokenizer(str(x), return_tensors=\"pt\")[\"input_ids\"].tolist())\n",
    "    \n",
    "    df_AmazonMovie_sub[\"description\"] = df_AmazonMovie_sub.description.map(lambda x: str(x))\n",
    "    df_AmazonMovie_sub[\"tokenized_word_id\"] = df_AmazonMovie_sub.description.map(lambda x: tokenizer(str(x), return_tensors=\"pt\")[\"input_ids\"].tolist())\n",
    "    \n",
    "    tfidf_weight = CalculateTFidf(df_movies, df_AmazonMovie_sub)\n",
    "    \n",
    "    document = df_movies.description.values\n",
    "    document2 = df_AmazonMovie_sub.description.values\n",
    "    documents = np.concatenate((document, document2), axis=0)\n",
    "\n",
    "    vec_list = []\n",
    "    for i in tqdm(range(len(documents))):\n",
    "        inputs = tokenizer(documents[i], return_tensors=\"pt\")\n",
    "        if inputs[\"input_ids\"].shape[1] <= 512: \n",
    "            outputs = net_1(**inputs)\n",
    "            doc_vec = torch.Tensor(tfidf_weight[i]).matmul(outputs.hidden_states[0][0]).data\n",
    "        else:\n",
    "            doc_vec = deal_long_document(inputs, i, net_1, tfidf_weight)\n",
    "        vec_list.append(list(doc_vec.data / inputs[\"input_ids\"].shape[1]))\n",
    "\n",
    "    vec_list_2 = []\n",
    "    for vector in vec_list:\n",
    "        vec_list_2.append([float(value) for value in vector])\n",
    "\n",
    "    dict_MLMovie2vec = dict(zip(df_movies.deal_id.values, vec_list_2[:len(df_movies.deal_id.values)]))\n",
    "    dict_AmazonMovie2vec = dict(zip(df_AmazonMovie_sub.deal_id.values, vec_list_2[len(df_movies.deal_id.values):]))\n",
    "    \n",
    "    #np.save(\"Dict_item2vec_A.npy\", dict_AmazonMovie2vec)\n",
    "    #np.save(\"Dict_item2vec_M.npy\", dict_MLMovie2vec)   \n",
    "\n",
    "    return dict_MLMovie2vec, dict_AmazonMovie2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T07:05:57.219932Z",
     "start_time": "2022-09-01T07:05:47.323465Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7dd15ce997b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"AM\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ML\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdict_itemId2vec_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_itemId2vec_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCalculate_item_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./Dictionary/MLtoAM/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-15d08ce5aba9>\u001b[0m in \u001b[0;36mCalculate_item_embedding\u001b[0;34m(source, target)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mdict_id2descripton\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/lizhi/CDRS-GNN/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Dict_MovieId2description.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mdf_movies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"description\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_movies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmovieId\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict_id2descripton\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mdf_movies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokenized_word_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_movies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mdf_AmazonMovie_sub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"description\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_AmazonMovie_sub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   3907\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3908\u001b[0m         \"\"\"\n\u001b[0;32m-> 3909\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3910\u001b[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001b[1;32m   3911\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"map\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0;31m# mapper is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-15d08ce5aba9>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mdict_id2descripton\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/lizhi/CDRS-GNN/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Dict_MovieId2description.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mdf_movies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"description\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_movies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmovieId\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict_id2descripton\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mdf_movies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokenized_word_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_movies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mdf_AmazonMovie_sub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"description\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_AmazonMovie_sub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2366\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2367\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2368\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2369\u001b[0m             )\n\u001b[1;32m   2370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2436\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2437\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2438\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2439\u001b[0m         )\n\u001b[1;32m   2440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m         )\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0mis_pretokenized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         )\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# domians = {\"ML\", \"AM\", \"AB\"}\n",
    "source = \"AM\"\n",
    "target = \"ML\"\n",
    "dict_itemId2vec_s, dict_itemId2vec_t = Calculate_item_embedding(source, target)\n",
    "\n",
    "save_path = \"./Dictionary/MLtoAM/\"\n",
    "\n",
    "np.save(save_path+\"Dict_item2vec_AMasT.npy\",dict_itemId2vec_t)\n",
    "np.save(save_path+\"Dict_item2vec_MLasS.npy\", dict_itemId2vec_s) \n",
    "\n",
    "dict_itemId2Cluster_s, dict_itemId2Cluster_t, dict_clusterId2vec = ItemClustering(dict_itemId2vec_s, dict_itemId2vec_t, 200)\n",
    "\n",
    "np.save(save_path+\"Dict_item2cluster_MLasS\",dict_itemId2Cluster_s)\n",
    "np.save(save_path+\"Dict_item2cluster_AMasT\",dict_itemId2Cluster_t)\n",
    "np.save(save_path+\"Dict_cluster2vec_MLtoAM\",dict_clusterId2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T07:28:50.295120Z",
     "start_time": "2022-09-01T07:28:47.726234Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-52c46a62bf4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"AM\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ML\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdict_itemId2vec_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_itemId2vec_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCalculate_item_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"./Dictionary/{source}to{target}/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-15d08ce5aba9>\u001b[0m in \u001b[0;36mCalculate_item_embedding\u001b[0;34m(source, target)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Download the pretrained BERT model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mnet_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBertForPreTraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mnet_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ML\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    960\u001b[0m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m                 \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    963\u001b[0m             )\n\u001b[1;32m    964\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \"\"\"\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m                 \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m             )\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# Load config dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1084\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1087\u001b[0m         )\n\u001b[1;32m   1088\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Linked-Etag\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'head'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m             )\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mssl_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mtls_in_tls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtls_in_tls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         )\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msend_sni\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         ssl_sock = _ssl_wrap_socket_impl(\n\u001b[0;32m--> 450\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtls_in_tls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m         )\n\u001b[1;32m    452\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         )\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m    868\u001b[0m                         \u001b[0;31m# non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda3/envs/torch_1.1.0/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# domians = {\"ML\", \"AM\", \"AB\"}\n",
    "source = \"AM\"\n",
    "target = \"ML\"\n",
    "dict_itemId2vec_s, dict_itemId2vec_t = Calculate_item_embedding(source, target)\n",
    "\n",
    "save_path = f\"./Dictionary/{source}to{target}/\"\n",
    "\n",
    "np.save(save_path+f\"Dict_item2vec_{target}asT.npy\",dict_itemId2vec_t)\n",
    "np.save(save_path+f\"Dict_item2vec_{source}asS.npy\", dict_itemId2vec_s) \n",
    "\n",
    "dict_itemId2Cluster_s, dict_itemId2Cluster_t, dict_clusterId2vec = ItemClustering(dict_itemId2vec_s, dict_itemId2vec_t, 200)\n",
    "\n",
    "np.save(save_path+f\"Dict_item2cluster_{source}asS\",dict_itemId2Cluster_s)\n",
    "np.save(save_path+f\"Dict_item2cluster_{target}asT\",dict_itemId2Cluster_t)\n",
    "np.save(save_path+f\"Dict_cluster2vec_{source}to{target}\",dict_clusterId2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T10:21:51.130550Z",
     "start_time": "2022-09-01T07:59:42.801054Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 47%|████▋     | 10689/22581 [11:50<1:12:35,  2.73it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 22581/22581 [2:21:04<00:00,  2.67it/s]   \n"
     ]
    }
   ],
   "source": [
    "# domians = {\"ML\", \"AM\", \"AB\"}\n",
    "source = \"AB\"\n",
    "target = \"ML\"\n",
    "dict_itemId2vec_s, dict_itemId2vec_t = Calculate_item_embedding_lastlayer(source, target)\n",
    "\n",
    "save_path = f\"./Dictionary/{source}to{target}/\"\n",
    "\n",
    "np.save(save_path+f\"Dict_item2vec_{target}asT_lastlayer.npy\",dict_itemId2vec_t)\n",
    "np.save(save_path+f\"Dict_item2vec_{source}asS_lastlayer.npy\", dict_itemId2vec_s) \n",
    "\n",
    "dict_itemId2Cluster_s, dict_itemId2Cluster_t, dict_clusterId2vec = ItemClustering(dict_itemId2vec_s, dict_itemId2vec_t, 200)\n",
    "\n",
    "np.save(save_path+f\"Dict_item2cluster_{source}asS_lastlayer\",dict_itemId2Cluster_s)\n",
    "np.save(save_path+f\"Dict_item2cluster_{target}asT_lastlayer\",dict_itemId2Cluster_t)\n",
    "np.save(save_path+f\"Dict_cluster2vec_{source}to{target}_lastlayer\",dict_clusterId2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T04:54:50.960439Z",
     "start_time": "2022-04-04T04:12:46.653825Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 24912/24912 [00:43<00:00, 568.79it/s] \n",
      "100%|██████████| 24912/24912 [39:20<00:00, 10.55it/s]  \n"
     ]
    }
   ],
   "source": [
    "# domians = {\"ML\", \"AM\", \"AB\"}\n",
    "source = \"ML\"\n",
    "target = \"AB\"\n",
    "dict_itemId2vec_s, dict_itemId2vec_t = Calculate_item_embedding(source, target)\n",
    "\n",
    "save_path = f\"./Dictionary/{source}to{target}/\"\n",
    "\n",
    "np.save(save_path+f\"Dict_item2vec_{target}asT.npy\",dict_itemId2vec_t)\n",
    "np.save(save_path+f\"Dict_item2vec_{source}asS.npy\", dict_itemId2vec_s) \n",
    "\n",
    "dict_itemId2Cluster_s, dict_itemId2Cluster_t, dict_clusterId2vec = ItemClustering(dict_itemId2vec_s, dict_itemId2vec_t, 200)\n",
    "\n",
    "np.save(save_path+f\"Dict_item2cluster_{source}asS\",dict_itemId2Cluster_s)\n",
    "np.save(save_path+f\"Dict_item2cluster_{target}asT\",dict_itemId2Cluster_t)\n",
    "np.save(save_path+f\"Dict_cluster2vec_{source}to{target}\",dict_clusterId2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T10:23:04.190839Z",
     "start_time": "2022-04-06T09:37:00.058578Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 18291/18291 [00:50<00:00, 360.65it/s]\n",
      "100%|██████████| 18291/18291 [42:55<00:00,  7.10it/s]  \n"
     ]
    }
   ],
   "source": [
    "# domians = {\"ML\", \"AM\", \"AB\"}\n",
    "source = \"AM\"\n",
    "target = \"AB\"\n",
    "dict_itemId2vec_s, dict_itemId2vec_t = Calculate_item_embedding_2(source, target)\n",
    "\n",
    "save_path = f\"./Dictionary/{source}to{target}/\"\n",
    "\n",
    "np.save(save_path+f\"Dict_item2vec_{target}asT.npy\",dict_itemId2vec_t)\n",
    "np.save(save_path+f\"Dict_item2vec_{source}asS.npy\", dict_itemId2vec_s) \n",
    "\n",
    "dict_itemId2Cluster_s, dict_itemId2Cluster_t, dict_clusterId2vec = ItemClustering(dict_itemId2vec_s, dict_itemId2vec_t, 200)\n",
    "\n",
    "np.save(save_path+f\"Dict_item2cluster_{source}asS\",dict_itemId2Cluster_s)\n",
    "np.save(save_path+f\"Dict_item2cluster_{target}asT\",dict_itemId2Cluster_t)\n",
    "np.save(save_path+f\"Dict_cluster2vec_{source}to{target}\",dict_clusterId2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-06T11:13:43.627793Z",
     "start_time": "2022-04-06T10:23:04.193221Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 19460/19460 [00:57<00:00, 336.49it/s]\n",
      "100%|██████████| 19460/19460 [47:08<00:00,  6.88it/s]  \n"
     ]
    }
   ],
   "source": [
    "# domians = {\"ML\", \"AM\", \"AB\"}\n",
    "source = \"AB\"\n",
    "target = \"AM\"\n",
    "dict_itemId2vec_s, dict_itemId2vec_t = Calculate_item_embedding_2(source, target)\n",
    "\n",
    "save_path = f\"./Dictionary/{source}to{target}/\"\n",
    "\n",
    "np.save(save_path+f\"Dict_item2vec_{target}asT.npy\",dict_itemId2vec_t)\n",
    "np.save(save_path+f\"Dict_item2vec_{source}asS.npy\", dict_itemId2vec_s) \n",
    "\n",
    "dict_itemId2Cluster_s, dict_itemId2Cluster_t, dict_clusterId2vec = ItemClustering(dict_itemId2vec_s, dict_itemId2vec_t, 200)\n",
    "\n",
    "np.save(save_path+f\"Dict_item2cluster_{source}asS\",dict_itemId2Cluster_s)\n",
    "np.save(save_path+f\"Dict_item2cluster_{target}asT\",dict_itemId2Cluster_t)\n",
    "np.save(save_path+f\"Dict_cluster2vec_{source}to{target}\",dict_clusterId2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset with only overlapping users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T08:23:35.925358Z",
     "start_time": "2022-05-10T08:23:35.886810Z"
    }
   },
   "outputs": [],
   "source": [
    "def Calculate_item_embedding_2(source, target):\n",
    "    # Download the pretrained BERT model.\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    net_1 = transformers.BertForPreTraining.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "    net_1.eval()\n",
    "    if source == \"AM\":\n",
    "        df_movies = pd.read_csv(\"./Datasets/Side_AmzMasS_wovlpu.csv\")\n",
    "    if source == \"AB\":\n",
    "        df_movies = pd.read_csv(\"./Datasets/Side_AmzBasS_wovlpu.csv\")     \n",
    "        \n",
    "    if target == \"AM\":\n",
    "        df_AmazonMovie_sub = pd.read_csv(\"./Datasets/Side_AmzMasT_wovlpu.csv\")\n",
    "    if target == \"AB\":\n",
    "        df_AmazonMovie_sub = pd.read_csv(\"./Datasets/Side_AmzBasT_wovlpu.csv\")         \n",
    "        \n",
    "    #dict_id2descripton = np.load(\"/home/lizhi/CDRS-GNN/\" + \"Dict_MovieId2description.npy\", allow_pickle=True).item()\n",
    "    df_movies[\"description\"] = df_movies.description.map(lambda x: str(x))\n",
    "    df_movies[\"tokenized_word_id\"] = df_movies.description.map(lambda x: tokenizer(str(x), return_tensors=\"pt\")[\"input_ids\"].tolist())\n",
    "    \n",
    "    df_AmazonMovie_sub[\"description\"] = df_AmazonMovie_sub.description.map(lambda x: str(x))\n",
    "    df_AmazonMovie_sub[\"tokenized_word_id\"] = df_AmazonMovie_sub.description.map(lambda x: tokenizer(str(x), return_tensors=\"pt\")[\"input_ids\"].tolist())\n",
    "    \n",
    "    tfidf_weight = CalculateTFidf(df_movies, df_AmazonMovie_sub)\n",
    "    \n",
    "    document = df_movies.description.values\n",
    "    document2 = df_AmazonMovie_sub.description.values\n",
    "    documents = np.concatenate((document, document2), axis=0)\n",
    "\n",
    "    vec_list = []\n",
    "    for i in tqdm(range(len(documents))):\n",
    "        inputs = tokenizer(documents[i], return_tensors=\"pt\")\n",
    "        if inputs[\"input_ids\"].shape[1] <= 512: \n",
    "            outputs = net_1(**inputs)\n",
    "            doc_vec = torch.Tensor(tfidf_weight[i]).matmul(outputs.hidden_states[0][0]).data\n",
    "        else:\n",
    "            doc_vec = deal_long_document(inputs, i, net_1, tfidf_weight)\n",
    "        vec_list.append(list(doc_vec.data / inputs[\"input_ids\"].shape[1]))\n",
    "\n",
    "    vec_list_2 = []\n",
    "    for vector in vec_list:\n",
    "        vec_list_2.append([float(value) for value in vector])\n",
    "\n",
    "    dict_MLMovie2vec = dict(zip(df_movies.deal_id.values, vec_list_2[:len(df_movies.deal_id.values)]))\n",
    "    dict_AmazonMovie2vec = dict(zip(df_AmazonMovie_sub.deal_id.values, vec_list_2[len(df_movies.deal_id.values):]))\n",
    "    \n",
    "    #np.save(\"Dict_item2vec_A.npy\", dict_AmazonMovie2vec)\n",
    "    #np.save(\"Dict_item2vec_M.npy\", dict_MLMovie2vec)   \n",
    "\n",
    "    return dict_MLMovie2vec, dict_AmazonMovie2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T08:23:36.496722Z",
     "start_time": "2022-05-10T08:23:36.480236Z"
    }
   },
   "outputs": [],
   "source": [
    "def ItemClustering(dict_itemId2vec_A, dict_itemId2vec_M, num_clusters):\n",
    "\n",
    "    dict_itemindex2vec_M = dict(zip(np.arange(len(dict_itemId2vec_M)), dict_itemId2vec_M.values()))\n",
    "    dict_itemindex2vec_A = dict(zip(np.arange(len(dict_itemId2vec_A)), dict_itemId2vec_A.values()))\n",
    "\n",
    "    dict_itemindex2ID_M = dict(zip(np.arange(len(dict_itemId2vec_M)), dict_itemId2vec_M.keys()))\n",
    "    dict_itemindex2ID_A = dict(zip(np.arange(len(dict_itemId2vec_A)), dict_itemId2vec_A.keys()))\n",
    "\n",
    "    dict_M_sample = {}\n",
    "    for item in range(len(dict_itemId2vec_M)):\n",
    "        dict_M_sample[item] = dict_itemindex2vec_M[item]\n",
    "    dict_A_sample = {}\n",
    "    for item in range(len(dict_itemId2vec_A)):\n",
    "        dict_A_sample[item] = dict_itemindex2vec_A[item]    \n",
    "\n",
    "    Jointed_items = list(dict_M_sample.values())\n",
    "    Jointed_items.extend(dict_A_sample.values())   \n",
    "    jointed_norm = preprocessing.normalize(Jointed_items, norm='l2')\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=6298).fit(jointed_norm)\n",
    "    lables = kmeans.predict(jointed_norm)\n",
    "\n",
    "    #gmm = GaussianMixture(n_components=num_clusters).fit(jointed_norm)\n",
    "    #lables = gmm.predict(jointed_norm)\n",
    "\n",
    "    Cluster_M = dict(pd.value_counts(lables[:len(dict_M_sample)]))\n",
    "    Cluster_A = dict(pd.value_counts(lables[len(dict_M_sample):]))\n",
    "\n",
    "    dict_itemId2Cluster_A = dict(zip(dict_itemId2vec_A.keys(), lables[len(dict_M_sample):]))\n",
    "    dict_itemId2Cluster_M = dict(zip(dict_itemId2vec_M.keys(), lables[:len(dict_M_sample)]))\n",
    "\n",
    "    #np.save(\"Dict_itemId2Cluster_T.npy\", dict_itemId2Cluster_A)\n",
    "    #np.save(\"Dict_itemId2Cluster_S.npy\", dict_itemId2Cluster_M)\n",
    "    \n",
    "    \n",
    "    count = np.zeros([num_clusters])\n",
    "    cluster_matrix = np.zeros([num_clusters,768])\n",
    "    for key, values in zip(dict_itemId2Cluster_A.keys(), dict_itemId2Cluster_A.values()):\n",
    "        cluster_matrix[values] += dict_itemId2vec_A[key]\n",
    "        count[values] += 1\n",
    "    for key, values in zip(dict_itemId2Cluster_M.keys(), dict_itemId2Cluster_M.values()):\n",
    "        cluster_matrix[values] += dict_itemId2vec_M[key]\n",
    "        count[values] += 1\n",
    "    \n",
    "    for i in range(num_clusters):\n",
    "        cluster_matrix[i] = cluster_matrix[i] / count[i]\n",
    "    dict_clusterId2vec = dict(zip(np.arange(num_clusters), cluster_matrix))\n",
    "    #np.save(\"Dict_clusterId2vec.npy\", dict_clusterId2vec)\n",
    "    \n",
    "    return dict_itemId2Cluster_A, dict_itemId2Cluster_M, dict_clusterId2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T06:50:50.058376Z",
     "start_time": "2022-05-12T06:28:55.792428Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 9704/9704 [00:24<00:00, 391.52it/s]\n",
      "100%|██████████| 9704/9704 [20:33<00:00,  7.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# domians = {\"ML\", \"AM\", \"AB\"}\n",
    "source = \"AM\"\n",
    "target = \"AB\"\n",
    "dict_itemId2vec_s, dict_itemId2vec_t = Calculate_item_embedding_2(source, target)\n",
    "\n",
    "save_path = f\"./Dictionary/{source}to{target}/\"\n",
    "\n",
    "np.save(save_path+f\"Dict_item2vec_{target}asT.npy\",dict_itemId2vec_t)\n",
    "np.save(save_path+f\"Dict_item2vec_{source}asS.npy\", dict_itemId2vec_s) \n",
    "\n",
    "dict_itemId2Cluster_s, dict_itemId2Cluster_t, dict_clusterId2vec = ItemClustering(dict_itemId2vec_s, dict_itemId2vec_t, 100)\n",
    "\n",
    "np.save(save_path+f\"Dict_item2cluster_{source}asS_wovlpu\",dict_itemId2Cluster_s)\n",
    "np.save(save_path+f\"Dict_item2cluster_{target}asT_wovlpu\",dict_itemId2Cluster_t)\n",
    "np.save(save_path+f\"Dict_cluster2vec_{source}to{target}_wovlpu\",dict_clusterId2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T07:06:03.916340Z",
     "start_time": "2022-05-12T06:50:50.060141Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 6231/6231 [00:16<00:00, 386.09it/s]\n",
      "100%|██████████| 6231/6231 [14:09<00:00,  7.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# domians = {\"ML\", \"AM\", \"AB\"}\n",
    "source = \"AB\"\n",
    "target = \"AM\"\n",
    "dict_itemId2vec_s, dict_itemId2vec_t = Calculate_item_embedding_2(source, target)\n",
    "\n",
    "save_path = f\"./Dictionary/{source}to{target}/\"\n",
    "\n",
    "np.save(save_path+f\"Dict_item2vec_{target}asT.npy\",dict_itemId2vec_t)\n",
    "np.save(save_path+f\"Dict_item2vec_{source}asS.npy\", dict_itemId2vec_s) \n",
    "\n",
    "dict_itemId2Cluster_s, dict_itemId2Cluster_t, dict_clusterId2vec = ItemClustering(dict_itemId2vec_s, dict_itemId2vec_t, 100)\n",
    "\n",
    "np.save(save_path+f\"Dict_item2cluster_{source}asS_wovlpu\",dict_itemId2Cluster_s)\n",
    "np.save(save_path+f\"Dict_item2cluster_{target}asT_wovlpu\",dict_itemId2Cluster_t)\n",
    "np.save(save_path+f\"Dict_cluster2vec_{source}to{target}_wovlpu\",dict_clusterId2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_1.1.0",
   "language": "python",
   "name": "torch_1.1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
